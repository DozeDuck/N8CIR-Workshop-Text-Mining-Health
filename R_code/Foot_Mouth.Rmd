---
title: "Text_Mining_Demo"
output: html_document
date: "`r Sys.Date()`"
author: Nadia Kennar 
toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Install and Load Packages

Lets go ahead and install all the necessary packages. 


```{r, echo=TRUE, eval=FALSE}
# Install
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
install.packages("syuzhet") # for sentiment analysis
install.packages("ggplot2") # for plotting graphs
install.packages("dplyr")  # for data manipulationn
install.packages("readr") # for reading in files
install.packages("tidytext") #for tokenisation, using dplyr, ggplot2 and other tidy tools 
install.packages("stringr") # for extracting matched patterns in strings
install.packages("sparklyr") # tokenizer package
install.packages("hunspell") # spell check tool
install.packages("textstem") # lemitisation
install.packages("udpipe") # POS tagging


```


and now load them in using the *library* function 


```{r}
# Load
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(dplyr)
library(readr)
library(tidytext)
library(stringr)
library(sparklyr)
library(hunspell)
library(textstem)
library(udpipe)

```



# Load in the dataset 

We are going to be using the 'text.csv' which was created once combining all of the rtf files into one dataset. 

```{r}
foot_mouth_df <- read_csv("code/data/foot_mouth/text.csv")
head(foot_mouth_df)
```



# Pre-processing 

Obviously, we can access any column, row or cell without using named labels. But it might be easier to give some of the things named labels. This makes more sense with columns - especially if we are going to split the columns into lots of other columns and it will be hard to keep track of what the numbered columns refer to.

```{r}
# Renaming columns 
names(foot_mouth_df)[1] <- "Number"
names(foot_mouth_df)[2] <- "Filename"
names(foot_mouth_df)[3] <- "Everything_else"
head(foot_mouth_df)
```


This file contains two types of recordings; one from diary entries and one from the interviews.

The diary files seem to start with "Information about diarist" while the interview files start with "date of interview".

It seems like we can't split the columns until we split this data frame into 2, one for diary files and one for interview files. How would you go about doing that? Steps to consider might include:

1) find the last row of the diary entries and the first row of the interview entries (using access rows and/or access cells)
2) save a new "diary" variable that contains all of the columns for all of the diary rows
3) save a new "interview" variable that contains all of the columns for all of the interview rows

So lets go ahead and split these file types, as they will be easier to work with

```{r}
#Splitting Datagrames
diary_file <- foot_mouth_df[1:39,]
group_int_file <- foot_mouth_df[40:87,]

head(diary_file)
```


Let's try extracting all of the dates with the format day/month/year from each row in this column. We can then place them ina new column called 'Dates'.

We access the column labelled 'everything_else' in our dataframe, and extract strings that match the RegEx pattern. Don't worry too much about the RegEx pattern for now, this is just to illustrate how it can be used. Then we assign these values to a new column labelled 'Dates' for the individual data frames (that is the 'diary_file' and the 'interview_file'). 

```{r}
#Adding Date Column 
  # first to the diary file 
diary_file$Date <- stringr::str_extract(diary_file$Everything_else, "(\\d{1,2}[/\\.-][ ]?)?(\\d{1,2}[ ]*[/\\.-]|January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Febr|Mar|Apr|Jun|Jul|Aug|Sept|Sep|Oct|Nov|Dec|Jan\\.|Feb\\.|Febr\\.|Mar\\.|Apr\\.|Jun\\.|Jul\\.|Aug\\.|Sept\\.|Sep\\.|Oct\\.|Nov\\.|Dec\\.)[ ]*[']?\\d{2,4}")

head(diary_file)

  # then to the group/interview files
group_int_file$Date <- stringr::str_extract(group_int_file$Everything_else, "(\\d{1,2}[/\\.-][ ]?)?(\\d{1,2}[ ]*[/\\.-]|January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Febr|Mar|Apr|Jun|Jul|Aug|Sept|Sep|Oct|Nov|Dec|Jan\\.|Feb\\.|Febr\\.|Mar\\.|Apr\\.|Jun\\.|Jul\\.|Aug\\.|Sept\\.|Sep\\.|Oct\\.|Nov\\.|Dec\\.)[ ]*[']?\\d{2,4}")

head(group_int_file)
```


I'm going to then join append these files using the rbind() function under the base package. 

```{r}
# Append files 
new_foot_mouth <- rbind(diary_file, group_int_file)
head(new_foot_mouth)
```

Another interesting piece of information to have, is the gender of the interview participant. Let's extract this information again by using RegEx...

```{r}
# Adding Gender Column and remove colon 
new_foot_mouth$Gender <- str_extract(new_foot_mouth$Everything_else, "M|F")
head(new_foot_mouth)
```

Now will also look at getting the Occupations!

```{r}
#Adding Occupation Column 
new_foot_mouth$Occupation <-as.integer(sub(".*?Group.*?(\\d+).*", "\\1", new_foot_mouth$Everything_else))
head(new_foot_mouth)
```




## Processing 

Processing steps

- Tokenisation, (or splitting text into various kinds of 'short things' that can be statistically analysed).
- Standardising the next (including converting uppercase to lower, correcting spelling, find-and-replace operations to remove abbreviations, etc.).
- Removing irrelevancies (anything from punctuation to stopwords like 'the' or 'to' that are unhelpful for many kinds of analysis).
- Consolidating (including stemming and lemmatisation that strip words back to their 'root').
- Basic NLP (that put some of the small things back together into logically useful medium things, like multi-word noun or verb phrases and proper names).

In practice, most text-mining work will require that any given corpus undergo multiple steps, but the exact steps and the exact order of steps depends on the desired analysis to be done. Thus, some of the examples that follow will use the raw text corpus as an input to the process while others use a processed corpus as an input.

As a side note, it is good practice to create new variables whenever you manipulate an existing variable rather than write over the original. This means that you keep the original and can go back to it anytime you need to if you want to try a different manipulation or correct an error. You will see how this works as we progress through the processing steps.

Since I am using this first type of processing to look for potential points for analysis, I will just stick to: word-tokenisation, basic standardisation and stemming


# Tokenisation 

Our first step is to cut our 'one big thing' into tokens, or 'lots of little things'. 

Whether you have one big file or many smaller ones, most text-mining work will also want to divide the corpus into what are known as 'tokens'. These 'tokens' are the unit of analysis, which might be chapters, sections, paragraphs, sentences, words, or something else.

Since we have one file already loaded as a corpus, we can skip the right to tokenising that text into sentences and words. Both options are functions available through the *tidtext* package that we imported earlier. These are both useful tokens in their own way, so we will see how to produce both kinds.

We start by dividing our corpus into words, splitting the string into substrings whenever 'word_tokenize' detects a word.

Let's try that. But this time, let's just have a look at the first 100 things it finds instead of the entire text. Run/Shift+Enter.

```{r}
#Since this method takes basically adding columns every time we do a new step, I will do processing on original foot_mouth_df
# Then I will append anything usefuul onto the modified 'new_foot_mouth' DataFrame.

# 1) Words 

token_list <- foot_mouth_df %>% tidytext::unnest_tokens(output = tokenised_words, 
                   token = "words",
                   input = Everything_else)
token_list

foot_mouth_token <- token_list %>% 
  aggregate(tokenised_words ~ Filename, FUN = function(token_list) sprintf("[%s]", toString(token_list)))

head(foot_mouth_token)
```


## Creating a word frequency 

The tidytext package provides a dataset called stop_words (what else) that contains a list of all the determiners and conjunctions, adverbs and adjectives that we can eliminate from a text, so we can analyse it properly.

```{r}
# We will create a function that will store all the operations to be able to plot the word frequency

word_frequency <- function(x, top = 10){
  x %>%
# We need a word count
  count(Word, sort = TRUE) %>%
# We want to create a factor from the word column with the levels showing the most frequent words as top level
# This is just for aestethic reasons, however, it helps make the point
  mutate(Word = factor(Word, levels = rev(unique(Word)))) %>% 
# We use the "top" variable defined in the function so we can decide how many words we want to use 
  top_n(top) %>%
# This will be useful later if we want to use a grouping variable and will do nothing if we don't  
  ungroup() %>%
# The graph itself
  ggplot(mapping = aes(x = Word, y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL)
}

reviews_tidy <- foot_mouth_df %>%
  tidytext::unnest_tokens("Word", "Everything_else") %>%
  dplyr::anti_join(tidytext::stop_words, by = c("Word" = "word")) %>% 
  mutate(Word = stringr::str_replace(Word, "'s", ""))

# Now you can plot
reviews_tidy %>%
  word_frequency(15) 

```



Once this is complete you can then apend the new dataframe that contains the tokenisaion words to our 'foot_mout_df'

```{r}
foot_mouth_df <- merge(foot_mouth_df, foot_mouth_token)

head(foot_mouth_df)
```



# Standardising 

If we want to focus on the 'bag of words' approach, we don't really care about uppercase or lowercase distinctions. For example, we want 'Privacy' to count as the same word as 'privacy', rather than as two different words.

We can remove all uppercase letters with the function 'tolower' under the base package 

However, you may have noticed that the above tokenisation code has already returned our text as lowercase, so we do not need to do anything more. But if you're curious, the code to do so is listed here!


```{r}
foot_mouth_df$lower_case <- 
tolower(foot_mouth_df$tokenised_words)
```




## Spell check 


We can use the hunspell_check and hunspell_suggest functions from the *hunspell* package to test individual words


```{r}
#foot_mouth_df$spell_checked <-
#hunspell(foot_mouth_df$Everything_else)

```






# Remove irrevalancies 

In this section we will identify how to remove;

1) punctuation 
2) empty spaces 
3) stop words 


### Removing Punctuation 

Punctuation is not always very useful for understanding text, especially if you look at words as tokens because lots of the punctuation ends up being tokenised on its own.

We could use RegEx to replace all punctuation with nothing, and that is a valid approach.


```{r}
# Removing Punction
foot_mouth_df$no_punct <- 
  str_replace_all(foot_mouth_df$tokenised_words, "[[:punct:]]", "")

head(foot_mouth_df$punct)

```


If you were interested in removing non-alphanumeric characters, the reg ex exprresion is as follows *"[^[:alnum:]]", ""*


```{r}
#Remove Non-Alphanumeric characters 

#foot_mouth_df$no_alpha_num <- 
  #str_replace_all(foot_mouth_df$Everything_else, "[^[:alnum:]]", "")
  
```


## Removing empty spaces 

Did you notice that removing the punctuation has left list items that are empty strings. Between 'corpus' and 'it', for example, is an item shown as ''. This is an empty string item that was a full stop before we removed the punctuation.

We can again use the str_replace_all function from the *stringry* package to replace all intances of empty spaces


```{r}

foot_mouth_df$no_spaces <- 
  str_replace_all(foot_mouth_df$no_punct, " ", repl ="")

head(foot_mouth_df)
```




### Removing Stop Words 

Stopwords are typically conjunctions ('and', 'or'), prepositions ('to', 'around'), determiners ('the', 'an'), possessives ('s) and the like. The are REALLY common in all languages, and tend to occur at about the same ratio in all kinds of writing, regardless of who did the writing or what it is about. These words are definitely important for structure as they make all the difference between "Freeze or I'll shoot!" and "Freeze and I'll shoot!".

Buuuut... Many for many text-mining analyses, especially those that take the bag of words approach, these words don't have a whole lot of meaning in and of themselves. Thus, we want to remove them.

For removing words in R it's a little awkward, but you can paste together a regex from tm's stopword list.



```{r}

stopwords_regex = paste(tm::stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')

foot_mouth_df$no_stop_words <-
  stringr::str_replace_all(foot_mouth_df$no_punct, stopwords_regex, '')

head(foot_mouth_df$no_stop_words)
```




# Consolidation

In this section we will look at 

1) stemming 
2) pos tagging 
3) lemmitisation 


### Stemming 

The tm package in R provides the stemDocument() function to stem the document to it's root.

This function either takes in a character vector and returns a character vector, or takes in a PlainTextDocument and returns a PlainTextDocument

```{r}

foot_mouth_df$stem_words <- 
  tm::stemDocument(foot_mouth_df$no_stop_words, language = "english")

head(foot_mouth_df)
```



## POS Tagging

Now it is time to tag that dataframe with POS-tags. This step is important because without POS-tags, everything by default will get treated like a noun.

This would then result in the Dataframe essentially going through a de-pluraliser, while all of the different verb tenses remain. So, before we lemmatise the Dataframe, we need to mark the corpus for part of speech tags, abbreviated to POS

Information on the udpipe package: Ready-made models for 65 languages trained on 101 treebanks from https://universaldependencies.org/ are provided to you. Some of these models were provided by the UDPipe community. Other models were build using this R package. You can either download these models manually in order to use it for annotation purposes or use udpipe_download_model to download these models for a specific language of choice. You have the following options:


```{r}
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(file = udmodel$file_model)


foot_mouth_df$pos_tags <- udpipe_annotate(udmodel, 
                     foot_mouth_df$no_spaces)



```



## Lemmatisation 

Lemmatisation is similar to stemming, in that it aims to turn various forms of the same word into a single form. However, lemmatisation is a bit more sophisticated because:

- It recognises irregular plurals and returns the correct singular form. Example = 'rocks' --> 'rock' but 'corpora' --> 'corpus'
- If part of speech tags are supplied, it treats verbs, adjectives and nouns differenly, even if they have the same surface form. Example - 'caring' would not be changed if used as an adjective (as in 'his caring manner') but would go to 'care' if it was a verb (as in 'he is caring for baby squirrels'. In contrast, stemming would remove the 'ing' and turn 'caring' into 'car'.
- If no part of speech tags are supplied, lemmatisation tools tend to assume words as nouns, so the process becomes a sophisticated de-pluraliser.
- This is better for this research because since we will be looking into the meaning of the data, it will need to put into the most accurate base form as possible, if I were to stem this, a lot of words would lose meaning!


```{r}
foot_mouth_df$lemm <- 
  textstem::lemmatize_words(foot_mouth_df$pos_tags)
```




# Append and Save new datafram 

```{r}
#proceessed_data <- left_join(new_foot_mouth, foot_mouth_df, by = c("Filename"))

#save(foot_mouth_df, file = "processed_fm_data.RData")
#load("processed_fm_data.RData"))
```











