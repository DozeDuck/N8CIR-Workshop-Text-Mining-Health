{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UKDS Logo](images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text-mining: Classifiers and sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sentiment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-Tools\" data-toc-modified-id=\"Sentiment-Analysis-Tools-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Sentiment Analysis Tools</a></span></li></ul></li><li><span><a href=\"#Foot-and-mouth-dataset-analysis-with-VADER\" data-toc-modified-id=\"Foot-and-mouth-dataset-analysis-with-VADER-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Foot and mouth dataset analysis with VADER</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filtering-the-Polarity-scores\" data-toc-modified-id=\"Filtering-the-Polarity-scores-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Filtering the Polarity scores</a></span></li><li><span><a href=\"#Analysis-on-Filtered-Sentiments\" data-toc-modified-id=\"Analysis-on-Filtered-Sentiments-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Analysis on Filtered Sentiments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-Polarity-Average\" data-toc-modified-id=\"Overall-Polarity-Average-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Overall Polarity Average</a></span></li><li><span><a href=\"#Sentiment-Analysis-by-Occupation\" data-toc-modified-id=\"Sentiment-Analysis-by-Occupation-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Sentiment Analysis by Occupation</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#VADER-Comparisons\" data-toc-modified-id=\"VADER-Comparisons-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>VADER Comparisons</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a table of contents provided here at the top of the notebook, but you can also access this menu at any point by clicking the Table of Contents button on the top toolbar (an icon with four horizontal bars, if unsure hover your mouse over the buttons). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Now that we have finished processing the data and put it into a nice, easy to read format it's onto the extraction! But first let's get all the necessary processing summary code ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported necessary modules\n",
      "   Unnamed: 0                0  \\\n",
      "0           0  5407diary02.rtf   \n",
      "1           1  5407diary03.rtf   \n",
      "2           2  5407diary07.rtf   \n",
      "3           3  5407diary08.rtf   \n",
      "4           4  5407diary09.rtf   \n",
      "5           5  5407diary10.rtf   \n",
      "6           6  5407diary13.rtf   \n",
      "7           7  5407diary14.rtf   \n",
      "8           8  5407diary15.rtf   \n",
      "9           9  5407diary16.rtf   \n",
      "\n",
      "                                                   1  \n",
      "0  \\n\\nInformation about diarist\\nDate of birth: ...  \n",
      "1  Information about diarist\\nDate of birth: 1966...  \n",
      "2  \\n\\nInformation about diarist\\nDate of birth: ...  \n",
      "3  Information about diarist\\nDate of birth: 1963...  \n",
      "4  Information about diarist\\nDate of birth: 1981...  \n",
      "5  Information about diarist\\nDate of birth: 1937...  \n",
      "6  Information about diarist\\nDate of birth: 1947...  \n",
      "7  \\nInformation about diarist\\nDate of birth: 19...  \n",
      "8  Information about diarist\\nDate of birth: 1949...  \n",
      "9  \\nInformation about diarist\\nDate of birth: 19...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# It is good practice to always start by importing the modules and packages you will need. \n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import re                         # re is for regular expressions, which we use later \n",
    "import pandas as pd               # we need pandas to import the foot_mouth_original.xls file\n",
    "! pip install xlrd                # apparently we also need xlrd to read the .xls file because pandas is not old school\n",
    "import xlrd                       # le sigh\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize    # importing the word_tokenize function from nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "!pip install autocorrect           # Spellchecker\n",
    "from autocorrect import Speller\n",
    "\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!\n",
    "\n",
    "foot_mouth_df = pd.read_csv ('../code/data/foot_mouth/text.csv')\n",
    "print (foot_mouth_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number         Filename                                    everything_else\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...\n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...\n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...\n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...\n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...\n",
      "Columns renamed :)\n",
      " \n",
      "   Number         Filename                                    everything_else  \\\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
      "\n",
      "  Occupation  \n",
      "0    Group 6  \n",
      "1    Group 6  \n",
      "2    Group 6  \n",
      "3    Group 6  \n",
      "4    Group 5  \n",
      " Occupation Dataframe Created!\n",
      " \n",
      " EVERYTHING READY! :)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "foot_mouth_df = pd.read_csv ('../code/data/foot_mouth/text.csv') \n",
    "\n",
    "foot_mouth_df.columns = [\"Number\", \"Filename\", \"everything_else\"]\n",
    "print(foot_mouth_df.head())\n",
    "\n",
    "print(\"Columns renamed :)\")\n",
    "print(\" \")\n",
    "\n",
    "#Creating New Dataframe with Occupation Column\n",
    "oc_foot_mouth = foot_mouth_df.assign(Occupation = foot_mouth_df['everything_else'].str.extract(r'(\\w+\\s+\\d{1,2})'))\n",
    "\n",
    "print(oc_foot_mouth.head()) # checking if it worked!\n",
    "\n",
    "print(\" Occupation Dataframe Created!\")\n",
    "\n",
    "print(\" \")\n",
    "print(\" EVERYTHING READY! :)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\n",
      "...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#Tokenising by word\n",
    "foot_mouth_df['tokenised_words'] = foot_mouth_df.apply(lambda row: nltk.word_tokenize(row['everything_else']), axis=1)\n",
    "\n",
    "#Removing Uppercase\n",
    "foot_mouth_df['txt_lower'] = foot_mouth_df['tokenised_words'].apply(lambda x: [w.lower() for w in x])\n",
    "\n",
    "#Correcting Spelling\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "foot_mouth_df['spell_checked'] = foot_mouth_df['txt_lower'].apply(lambda x: [spell(w) for w in x])\n",
    "\n",
    "def replacement_mapping(x):\n",
    "        if x == \"der\":\n",
    "            return re.sub(\"der\",\"defra\",x)\n",
    "        else:\n",
    "            return x  \n",
    "\n",
    "def replacement_mapping_2(x):\n",
    "        if x == \"ffd\":\n",
    "            return re.sub(\"ffd\",\"fmd\",x)\n",
    "        else:\n",
    "            return x  \n",
    "        \n",
    "foot_mouth_df[\"spell_checked\"] = foot_mouth_df[\"spell_checked\"].apply(lambda x:[replacement_mapping(w) for w in x])\n",
    "foot_mouth_df[\"spell_checked\"] = foot_mouth_df[\"spell_checked\"].apply(lambda x:[replacement_mapping_2(w) for w in x])\n",
    "\n",
    "#Removing Punctuation and getting rid of resulting space\n",
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\"      # Define a variable with all the punctuation to remove.\n",
    "print(English_punctuation)                                     # Print that defined variable, just to check it is correct.\n",
    "print(\"...\") \n",
    "\n",
    "\n",
    "def remove_punctuation(from_text):                           # Had to define a function to iterate over the strings in a row\n",
    "    table = str.maketrans('', '', English_punctuation)       # The python function 'maketrans' creates a table that maps\n",
    "    stripped = [w.translate(table) for w in from_text]        # the punctation marks to 'None'. Print the table to check. \n",
    "    return stripped\n",
    "\n",
    "foot_mouth_df['no_punct'] = [remove_punctuation(i) for i in foot_mouth_df['spell_checked']] # Iterating above function to each\n",
    "\n",
    "foot_mouth_df['no_punct_no_space'] = [list(filter(None, sublist)) for sublist in foot_mouth_df['no_punct']]\n",
    "\n",
    "#POS Tagging and Lemmatisation\n",
    "\n",
    "foot_mouth_df['pos_tag'] = foot_mouth_df['no_punct_no_space'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "foot_mouth_df['lemmatised'] = foot_mouth_df['pos_tag'].apply(lambda x: [lemmatizer.lemmatize(y[0], get_wordnet_pos(y[0])) for y in x])\n",
    "\n",
    "#Removing Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                                                                                                                   \n",
    "new_stopwords =[e for e in stop_words if e not in (\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",'no','not',\"only\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\")]\n",
    "foot_mouth_df['no_stop_words'] = foot_mouth_df['lemmatised'].apply(lambda x: [item for item in x if item not in new_stopwords])                        \n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number         Filename                                    everything_else  \\\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
      "\n",
      "                                     tokenised_words  \\\n",
      "0  [Information, about, diarist, Date, of, birth,...   \n",
      "1  [Information, about, diarist, Date, of, birth,...   \n",
      "2  [Information, about, diarist, Date, of, birth,...   \n",
      "3  [Information, about, diarist, Date, of, birth,...   \n",
      "4  [Information, about, diarist, Date, of, birth,...   \n",
      "\n",
      "                                           txt_lower  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                       spell_checked  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                            no_punct  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                   no_punct_no_space  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                             pos_tag  \\\n",
      "0  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "1  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "2  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "3  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "4  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "\n",
      "                                          lemmatised  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                       no_stop_words  \n",
      "0  [information, diarist, date, birth, 1975, gend...  \n",
      "1  [information, diarist, date, birth, 1966, gend...  \n",
      "2  [information, diarist, date, birth, 1964, gend...  \n",
      "3  [information, diarist, date, birth, 1963, gend...  \n",
      "4  [information, diarist, date, birth, 1981, gend...  \n",
      " \n",
      " \n",
      "All Set for Extraction!\n"
     ]
    }
   ],
   "source": [
    "# Appending final processed data column onto the oc_foot_mouth Dataframe\n",
    "\n",
    "processed = foot_mouth_df['no_stop_words']\n",
    "\n",
    "oc_foot_mouth = oc_foot_mouth.join(processed)\n",
    "oc_foot_mouth.rename(columns = {'no_stop_words':'processed_text'}, inplace = True)\n",
    "\n",
    "print(foot_mouth_df.head())\n",
    "print(\" \")\n",
    "oc_foot_mouth.head()\n",
    "\n",
    "print(\" \")\n",
    "print(\"All Set for Extraction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "\n",
       "  Occupation                                     processed_text  \n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...  \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...  \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...  \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...  \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...  \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first (and main) step in the extraction process will be Sentiment Analysis. Sentiment analysis is a commonly used example of automatic classification. To be clear, automatic classification means that a model or learning algorithm has been trained on correctly classified documents and it uses this training to return a probability assessment of what class a new document should belong to. \n",
    "\n",
    "Sentiment analysis works the same way, but usually only has two classes - positive and negative. A trained model looks at new data and says whether that new data is likely to be positive or negative and this is what we will be using to conduct our analysis. Let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by importing and downloading some useful packages, including `VADER`: it is based on `nltk` and has built in sentiment analysis tools. \n",
    "\n",
    "To import the packages, click in the code cell below and hit the 'Run' button at the top of this page or by holding down the 'Shift' key and hitting the 'Enter' key. \n",
    "\n",
    "For the rest of this notebook, I will use 'Run/Shift+Enter' as short hand for 'click in the code cell below and hit the 'Run' button at the top of this page or by hold down the 'Shift' key while hitting the 'Enter' key'. \n",
    "\n",
    "Run/Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER works by giving the negative `neg` score, neutral `neu` score, and positive `pos` score along with the overal `compound` polarity score! (see example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.8553}\n",
      "{'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.7644}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(\"Textblob is just super. I love it!\"))\n",
    "print(sia.polarity_scores(\"Cabbages are the worst. Say no to cabbages!\"))\n",
    "print(sia.polarity_scores(\"Paris is the capital of France\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to then find JUST the overall polarity, you can create and use the following `get_scores` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8553\n"
     ]
    }
   ],
   "source": [
    "def get_scores(content):\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    return sia_scores['compound']\n",
    "\n",
    "print(get_scores(\"Textblob is just super. I love it!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foot and mouth dataset analysis with VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super. Now let's do this with our foot and mouth Pandas DataFrame. For this we will be looking at the polarity of Groups 1 and 4 in comparison to the average across occupations!\n",
    "\n",
    "First, we will use the previously mentioned `get_scores` function to calculate the polarity of each file/row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                          polarities  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(content):\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    return sia_scores['compound']\n",
    "\n",
    "oc_foot_mouth['polarities'] = oc_foot_mouth['processed_text'].apply(lambda x : [get_scores(y)for y in x])\n",
    "\n",
    "oc_foot_mouth[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1027,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1779,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3818,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.128,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3612,\n",
       " -0.4019,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " -0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2263,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " -0.6597,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0772,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4019,\n",
       " -0.1531,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5859,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2263,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4588,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1531,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1531,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6369,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4939,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0772,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1027,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3818,\n",
       " -0.3818,\n",
       " -0.5719,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1531,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3818,\n",
       " 0.0,\n",
       " -0.6486,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3818,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0772,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3182,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.128,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5267,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6486,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6908,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5106,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5106,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3182,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " 0.128,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4019,\n",
       " 0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4588,\n",
       " -0.2732,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5106,\n",
       " 0.0772,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1779,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5423,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6369,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2263,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4404,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4588,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.296,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4019,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.296,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.296,\n",
       " 0.0,\n",
       " -0.2023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.34,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0772,\n",
       " 0.0,\n",
       " 0.5859,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking more closely at the polarity values in one file \n",
    "oc_foot_mouth.loc[1,'polarities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it looks like there is an issue with there being too many neutral words in this dataset that could skew the results (for example whole phrases such as \" Information about diarist\" or \"date of birth\" that are just documentations and do not have anything to do with the actual responses submitted by the participants). So we will need to filter out all of these neutral scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarities</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.296, 0.4588, 0.2732, -0.296, -0.6249, 0.361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.1027, 0.3818, -0.5719, -0.4767, -0.1779, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4404, 0.3182, 0.296, -0.3612, -0.296, -0.54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.6369, 0.6369, 0.128, 0.4404, -0.5423, -0.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.128, 0.2732, 0.0772, -0.4404, 0.25, 0.0772...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.4215, -0.5859, -0.4588, -0.296, 0.2023, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.3612, -0.4019, 0.4939, 0.4939, 0.3182, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.4939, 0.3818, 0.5719, 0.4019, -0.5423, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4215, 0.4404, 0.4019, 0.4404, -0.4019, 0.40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.4939, 0.4939, 0.4767, 0.4404, 0.401...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                          polarities  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            filtered  \n",
       "0  [0.296, 0.4588, 0.2732, -0.296, -0.6249, 0.361...  \n",
       "1  [-0.1027, 0.3818, -0.5719, -0.4767, -0.1779, -...  \n",
       "2  [0.4404, 0.3182, 0.296, -0.3612, -0.296, -0.54...  \n",
       "3  [0.6369, 0.6369, 0.128, 0.4404, -0.5423, -0.31...  \n",
       "4  [-0.128, 0.2732, 0.0772, -0.4404, 0.25, 0.0772...  \n",
       "5  [-0.4215, -0.5859, -0.4588, -0.296, 0.2023, -0...  \n",
       "6  [-0.3612, -0.4019, 0.4939, 0.4939, 0.3182, -0....  \n",
       "7  [-0.4939, 0.3818, 0.5719, 0.4019, -0.5423, 0.0...  \n",
       "8  [0.4215, 0.4404, 0.4019, 0.4404, -0.4019, 0.40...  \n",
       "9  [-0.25, -0.4939, 0.4939, 0.4767, 0.4404, 0.401...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth['filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['polarities']]\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much better! Now lets do the analysis on these new scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Filtered Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will just be calculating the means of the scores! First though we will need to import numpy so we can use its mean function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be creating a new column for the mean polarity score for each row/file - this will make thngs easier for the next steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarities</th>\n",
       "      <th>filtered</th>\n",
       "      <th>pol_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.296, 0.4588, 0.2732, -0.296, -0.6249, 0.361...</td>\n",
       "      <td>0.095934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.1027, 0.3818, -0.5719, -0.4767, -0.1779, -...</td>\n",
       "      <td>0.020041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4404, 0.3182, 0.296, -0.3612, -0.296, -0.54...</td>\n",
       "      <td>0.195452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.6369, 0.6369, 0.128, 0.4404, -0.5423, -0.31...</td>\n",
       "      <td>0.079971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.128, 0.2732, 0.0772, -0.4404, 0.25, 0.0772...</td>\n",
       "      <td>0.143990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.4215, -0.5859, -0.4588, -0.296, 0.2023, -0...</td>\n",
       "      <td>-0.030438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.3612, -0.4019, 0.4939, 0.4939, 0.3182, -0....</td>\n",
       "      <td>0.073972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.4939, 0.3818, 0.5719, 0.4019, -0.5423, 0.0...</td>\n",
       "      <td>0.103030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4215, 0.4404, 0.4019, 0.4404, -0.4019, 0.40...</td>\n",
       "      <td>0.192486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.4939, 0.4939, 0.4767, 0.4404, 0.401...</td>\n",
       "      <td>0.101971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                          polarities  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            filtered  pol_mean  \n",
       "0  [0.296, 0.4588, 0.2732, -0.296, -0.6249, 0.361...  0.095934  \n",
       "1  [-0.1027, 0.3818, -0.5719, -0.4767, -0.1779, -...  0.020041  \n",
       "2  [0.4404, 0.3182, 0.296, -0.3612, -0.296, -0.54...  0.195452  \n",
       "3  [0.6369, 0.6369, 0.128, 0.4404, -0.5423, -0.31...  0.079971  \n",
       "4  [-0.128, 0.2732, 0.0772, -0.4404, 0.25, 0.0772...  0.143990  \n",
       "5  [-0.4215, -0.5859, -0.4588, -0.296, 0.2023, -0... -0.030438  \n",
       "6  [-0.3612, -0.4019, 0.4939, 0.4939, 0.3182, -0....  0.073972  \n",
       "7  [-0.4939, 0.3818, 0.5719, 0.4019, -0.5423, 0.0...  0.103030  \n",
       "8  [0.4215, 0.4404, 0.4019, 0.4404, -0.4019, 0.40...  0.192486  \n",
       "9  [-0.25, -0.4939, 0.4939, 0.4767, 0.4404, 0.401...  0.101971  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth['pol_mean'] = oc_foot_mouth['filtered'].apply(np.mean)\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It appears that sentiments were neutral overall even with them being fairly subjective. This is quite interesting given how sad the topic is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Polarity Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09082551868902182\n"
     ]
    }
   ],
   "source": [
    "print(oc_foot_mouth['pol_mean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis by Occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do our sentiment analysis on Groups 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use the same `==` operator used in the pre-processing stage to filter the dataframe by Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 1']\n",
    "group4_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a quick check to see if the Dataframe has been filtered correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number         Filename  \\\n",
      "33      33  5407diary47.rtf   \n",
      "34      34  5407diary48.rtf   \n",
      "35      35  5407diary49.rtf   \n",
      "36      36  5407diary52.rtf   \n",
      "37      37  5407diary53.rtf   \n",
      "38      38  5407diary54.rtf   \n",
      "40      40     5407fg01.rtf   \n",
      "80      80    5407int47.rtf   \n",
      "81      81    5407int48.rtf   \n",
      "82      82    5407int49.rtf   \n",
      "83      83    5407int52.rtf   \n",
      "84      84    5407int53.rtf   \n",
      "85      85    5407int54.rtf   \n",
      "\n",
      "                                      everything_else Occupation  \\\n",
      "33  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "34  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "35  \\t\\nInformation about diarist\\nDate of birth: ...    Group 1   \n",
      "36  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "37  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "38  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "40  \\nGroups Discussion with Members of  Farmers F...    Group 1   \n",
      "80  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "81  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "82  \\nDate of Interview: 22/01/02\\n\\nInformation a...    Group 1   \n",
      "83  \\nDate of Interview: 08/01/02\\n\\nInformation a...    Group 1   \n",
      "84  \\nDate of Interview: 21/01/02\\n\\nInformation a...    Group 1   \n",
      "85  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "\n",
      "                                       processed_text  \\\n",
      "33  [information, diarist, date, birth, 1956, gend...   \n",
      "34  [information, diarist, date, birth, 1948, gend...   \n",
      "35  [information, diarist, date, birth, 1957, gend...   \n",
      "36  [information, diarist, date, birth, 1969, gend...   \n",
      "37  [information, diarist, date, birth, 1952, gend...   \n",
      "38  [information, diarist, date, birth, 1943, gend...   \n",
      "40  [group, discussion, member, farmer, farmer, wo...   \n",
      "80  [date, interview, 170102, information, panel, ...   \n",
      "81  [date, interview, 170102, information, panel, ...   \n",
      "82  [date, interview, 220102, information, panel, ...   \n",
      "83  [date, interview, 080102, information, panel, ...   \n",
      "84  [date, interview, 210102, information, panel, ...   \n",
      "85  [date, interview, 170102, information, panel, ...   \n",
      "\n",
      "                                           polarities  \\\n",
      "33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "34  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "35  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "36  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "37  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "38  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "40  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "80  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "81  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "82  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "83  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "84  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "85  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                             filtered  pol_mean  \n",
      "33  [0.5719, 0.4019, 0.4019, 0.1779, 0.3182, -0.42...  0.127358  \n",
      "34  [0.296, -0.5423, 0.4019, 0.3182, 0.4215, -0.29...  0.092334  \n",
      "35  [0.4767, 0.3612, 0.4215, 0.2732, 0.4404, 0.476...  0.157370  \n",
      "36  [0.3612, 0.4019, -0.296, 0.3612, 0.34, -0.34, ...  0.147143  \n",
      "37  [0.4939, -0.2732, 0.3612, 0.4404, 0.0772, 0.54...  0.116682  \n",
      "38  [-0.4939, -0.4939, 0.4588, -0.1531, -0.296, -0...  0.055384  \n",
      "40  [0.2263, 0.4019, -0.296, 0.4019, -0.4767, 0.44...  0.045139  \n",
      "80  [-0.6249, -0.1531, 0.2732, -0.1531, -0.6808, -...  0.025388  \n",
      "81  [0.5574, -0.296, 0.5574, 0.296, 0.4939, 0.4019...  0.099352  \n",
      "82  [-0.1531, 0.2732, -0.5423, -0.296, -0.4019, -0...  0.067526  \n",
      "83  [-0.6249, -0.1531, -0.4404, 0.0772, 0.1531, -0...  0.089924  \n",
      "84  [0.4939, -0.3818, 0.2732, -0.34, 0.4939, -0.20...  0.104325  \n",
      "85  [0.296, 0.296, 0.2732, 0.296, 0.296, -0.5574, ...  0.138427  \n",
      " \n",
      "length is 13\n"
     ]
    }
   ],
   "source": [
    "#For group 1\n",
    "print(group1_foot.loc[:])\n",
    "\n",
    "print(\" \")\n",
    "print(\"length is \" + str(len(group1_foot))) # length should be 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number         Filename  \\\n",
      "12      12  5407diary19.rtf   \n",
      "13      13  5407diary21.rtf   \n",
      "14      14  5407diary22.rtf   \n",
      "15      15  5407diary23.rtf   \n",
      "16      16  5407diary24.rtf   \n",
      "17      17  5407diary26.rtf   \n",
      "32      32  5407diary44.rtf   \n",
      "43      43     5407fg04.rtf   \n",
      "58      58    5407int19.rtf   \n",
      "59      59    5407int20.rtf   \n",
      "60      60    5407int21.rtf   \n",
      "61      61    5407int22.rtf   \n",
      "62      62    5407int23.rtf   \n",
      "63      63    5407int24.rtf   \n",
      "64      64    5407int26.rtf   \n",
      "79      79    5407int44.rtf   \n",
      "\n",
      "                                      everything_else Occupation  \\\n",
      "12  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "13  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "14  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "15  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "16  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "17  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "32  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "43  \\nNO AUDIO RECORDING\\n\\nGroups Discussion with...    Group 4   \n",
      "58  \\nDate of Interview: 23/02/02\\n\\nInformation a...    Group 4   \n",
      "59  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "60  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "61  \\nDate of Interview: 27/02/02\\n\\nInformation a...    Group 4   \n",
      "62  \\nDate of Interview: 21/02/02\\n\\nInformation a...    Group 4   \n",
      "63  \\nDate of Interview: 21/02/02\\n\\nInformation a...    Group 4   \n",
      "64  \\nDate of Interview: 01/03/02\\n\\nInformation a...    Group 4   \n",
      "79  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "\n",
      "                                       processed_text  \\\n",
      "12  [information, diarist, date, birth, 1953, gend...   \n",
      "13  [information, diarist, date, birth, 1971, gend...   \n",
      "14  [information, diarist, date, birth, 1961, gend...   \n",
      "15  [information, diarist, date, birth, 1965, gend...   \n",
      "16  [information, diarist, date, birth, 1959, gend...   \n",
      "17  [information, diarist, date, birth, 1971, gend...   \n",
      "32  [information, diarist, date, birth, 1962, gend...   \n",
      "43  [no, audio, record, group, discussion, member,...   \n",
      "58  [date, interview, 230202, information, panel, ...   \n",
      "59  [date, interview, 260202, information, panel, ...   \n",
      "60  [date, interview, 260202, information, panel, ...   \n",
      "61  [date, interview, 270202, information, panel, ...   \n",
      "62  [date, interview, 210202, information, panel, ...   \n",
      "63  [date, interview, 210202, information, panel, ...   \n",
      "64  [date, interview, 010302, information, panel, ...   \n",
      "79  [date, interview, 260202, information, panel, ...   \n",
      "\n",
      "                                           polarities  \\\n",
      "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "13  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "14  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "15  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "16  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "32  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "43  [-0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n",
      "58  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "59  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "60  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "61  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "62  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "63  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "64  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "79  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                             filtered  pol_mean  \n",
      "12  [0.4019, -0.34, 0.4019, -0.34, -0.0772, -0.381...  0.114117  \n",
      "13  [0.3612, 0.0772, 0.0772, 0.3612, -0.3818, 0.44...  0.130873  \n",
      "14  [0.4404, -0.5574, -0.2023, -0.296, -0.296, 0.4...  0.133488  \n",
      "15  [0.3182, 0.0772, -0.4939, -0.2732, -0.4404, -0...  0.126105  \n",
      "16  [0.6249, 0.0772, -0.5423, 0.4939, -0.34, -0.51...  0.038357  \n",
      "17  [-0.4939, 0.4404, -0.4019, -0.4215, 0.3612, -0...  0.159259  \n",
      "32  [-0.4215, 0.2263, 0.3612, -0.2732, 0.128, 0.12... -0.119248  \n",
      "43  [-0.296, 0.0772, 0.296, -0.6249, -0.6249, 0.29... -0.008838  \n",
      "58  [0.4019, 0.4588, -0.34, 0.3612, -0.34, 0.296, ...  0.081429  \n",
      "59  [0.25, 0.4019, 0.4215, -0.2732, 0.2732, -0.273...  0.046174  \n",
      "60  [-0.3182, 0.25, 0.3818, 0.4939, -0.1027, 0.361...  0.106792  \n",
      "61  [0.2732, 0.3182, -0.296, 0.3182, 0.4019, 0.401...  0.086690  \n",
      "62  [0.4588, 0.5106, -0.34, -0.2732, 0.4404, -0.22...  0.044280  \n",
      "63  [-0.2023, 0.6249, 0.1027, 0.3612, 0.1531, -0.0...  0.035417  \n",
      "64  [0.3612, 0.3182, 0.4019, 0.5106, -0.296, 0.34,...  0.084794  \n",
      "79  [0.4019, -0.25, 0.0772, 0.3818, 0.4404, 0.4939...  0.174957  \n",
      " \n",
      "length is 16\n"
     ]
    }
   ],
   "source": [
    "#For group 4\n",
    "print(group4_foot.loc[:])\n",
    "\n",
    "print(\" \")\n",
    "print(\"length is \" + str(len(group4_foot))) # length should be 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now let's do Sentiment analysis on these new filtered Dataframes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Group 1 averages are: 0.09741171618892061\n",
      " \n",
      " Group 4 averages are: 0.07716526516106408\n"
     ]
    }
   ],
   "source": [
    "print(\" Group 1 averages are: \" + str (group1_foot[\"pol_mean\"].mean()))\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \" + str(group4_foot[\"pol_mean\"].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok not bad! Although it does definitely look like things are not that much different to the general average!\n",
    "\n",
    "Now for visualisations purposes I will get the averages for the other occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 2']\n",
    "group3_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 3']\n",
    "group5_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 5']\n",
    "group6_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09077748021238714\n",
      " \n",
      "0.08688578256118008\n",
      " \n",
      "0.10130420217906953\n",
      " \n",
      "0.09098646502316536\n"
     ]
    }
   ],
   "source": [
    "print(group2_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(group3_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(group5_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(group6_foot[\"pol_mean\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the proper values for everything we can summarise them all in one cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score averages are: \n",
      "0.09082551868902182\n",
      " \n",
      " Group 1 averages are: \n",
      "0.09741171618892061\n",
      " \n",
      " Group 4 averages are: \n",
      "0.07716526516106408\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.09077748021238714\n",
      " \n",
      "Group3:\n",
      "0.08688578256118008\n",
      " \n",
      "Group5:\n",
      "0.10130420217906953\n",
      " \n",
      "Group6:\n",
      "0.09098646502316536\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth[\"pol_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group3:\")\n",
    "print(group3_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group5:\")\n",
    "print(group5_foot[\"pol_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot[\"pol_mean\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we will save the resulting Dataframe into a csv file so that we can export it into an R notebook for summarisation, visualisation and statistical testing purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\L_Pel\\\\OneDrive\\\\Documents\\\\GitHub\\\\text-mining-private\\\\code'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth.to_csv('C:/Users/L_Pel/OneDrive/Documents/GitHub/text-mining-private/code/data/foot_mouth_analysed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
