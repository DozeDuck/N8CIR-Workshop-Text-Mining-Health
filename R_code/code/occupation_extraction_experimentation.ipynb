{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UKDS Logo](images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text-mining: Classifiers and sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sentiment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-Tools\" data-toc-modified-id=\"Sentiment-Analysis-Tools-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Sentiment Analysis Tools</a></span></li></ul></li><li><span><a href=\"#Analyse-trivial-documents-with-built-in-sentiment-analysis-tool\" data-toc-modified-id=\"Analyse-trivial-documents-with-built-in-sentiment-analysis-tool-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Analyse trivial documents with built-in sentiment analysis tool</a></span></li><li><span><a href=\"#Analyse-foot-and-mouth-dataset\" data-toc-modified-id=\"Analyse-foot-and-mouth-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Analyse foot and mouth dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-Average-Polarity-and-Subjectivity\" data-toc-modified-id=\"Overall-Average-Polarity-and-Subjectivity-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Overall Average Polarity and Subjectivity</a></span></li><li><span><a href=\"#Sentiment-Analysis-by-Occupation\" data-toc-modified-id=\"Sentiment-Analysis-by-Occupation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sentiment Analysis by Occupation</a></span></li></ul></li><li><span><a href=\"#Sorting-out-Sentiment-Score-Issues\" data-toc-modified-id=\"Sorting-out-Sentiment-Score-Issues-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sorting out Sentiment Score Issues</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspecting-Sentiment-and-Polarity-by-Row\" data-toc-modified-id=\"Inspecting-Sentiment-and-Polarity-by-Row-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Inspecting Sentiment and Polarity by Row</a></span></li><li><span><a href=\"#Filtering-the-Sentiment-Scores\" data-toc-modified-id=\"Filtering-the-Sentiment-Scores-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Filtering the Sentiment Scores</a></span></li></ul></li><li><span><a href=\"#Analysis-on-Filtered-Sentiments\" data-toc-modified-id=\"Analysis-on-Filtered-Sentiments-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analysis on Filtered Sentiments</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overall-Score-Average\" data-toc-modified-id=\"Overall-Score-Average-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Overall Score Average</a></span></li><li><span><a href=\"#Occupation-Sentiment-Averages\" data-toc-modified-id=\"Occupation-Sentiment-Averages-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Occupation Sentiment Averages</a></span></li></ul></li><li><span><a href=\"#Textblob-on-Different-dataset-formats\" data-toc-modified-id=\"Textblob-on-Different-dataset-formats-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Textblob on Different dataset formats</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analysis-On-Raw-Data\" data-toc-modified-id=\"Analysis-On-Raw-Data-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Analysis On Raw Data</a></span></li><li><span><a href=\"#Analysis-on-Sentences\" data-toc-modified-id=\"Analysis-on-Sentences-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Analysis on Sentences</a></span></li></ul></li><li><span><a href=\"#Using-VADER\" data-toc-modified-id=\"Using-VADER-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Using VADER</a></span><ul class=\"toc-item\"><li><span><a href=\"#On-the-Raw-Data\" data-toc-modified-id=\"On-the-Raw-Data-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>On the Raw Data</a></span></li><li><span><a href=\"#On-Sentence-Tokenised-data\" data-toc-modified-id=\"On-Sentence-Tokenised-data-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>On Sentence Tokenised data</a></span></li><li><span><a href=\"#On-Fully-Processed-Data\" data-toc-modified-id=\"On-Fully-Processed-Data-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>On Fully-Processed Data</a></span></li></ul></li><li><span><a href=\"#VADER-Comparisons\" data-toc-modified-id=\"VADER-Comparisons-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>VADER Comparisons</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There is a table of contents provided here at the top of the notebook, but you can also access this menu at any point by clicking the Table of Contents button on the top toolbar (an icon with four horizontal bars, if unsure hover your mouse over the buttons). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Now that we have finished processing the data and put it into a nice, easy to read format it's onto the extraction! But first let's get all the necessary processing summary code ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported necessary modules\n",
      "   Unnamed: 0                0  \\\n",
      "0           0  5407diary02.rtf   \n",
      "1           1  5407diary03.rtf   \n",
      "2           2  5407diary07.rtf   \n",
      "3           3  5407diary08.rtf   \n",
      "4           4  5407diary09.rtf   \n",
      "5           5  5407diary10.rtf   \n",
      "6           6  5407diary13.rtf   \n",
      "7           7  5407diary14.rtf   \n",
      "8           8  5407diary15.rtf   \n",
      "9           9  5407diary16.rtf   \n",
      "\n",
      "                                                   1  \n",
      "0  \\n\\nInformation about diarist\\nDate of birth: ...  \n",
      "1  Information about diarist\\nDate of birth: 1966...  \n",
      "2  \\n\\nInformation about diarist\\nDate of birth: ...  \n",
      "3  Information about diarist\\nDate of birth: 1963...  \n",
      "4  Information about diarist\\nDate of birth: 1981...  \n",
      "5  Information about diarist\\nDate of birth: 1937...  \n",
      "6  Information about diarist\\nDate of birth: 1947...  \n",
      "7  \\nInformation about diarist\\nDate of birth: 19...  \n",
      "8  Information about diarist\\nDate of birth: 1949...  \n",
      "9  \\nInformation about diarist\\nDate of birth: 19...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# It is good practice to always start by importing the modules and packages you will need. \n",
    "\n",
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import re                         # re is for regular expressions, which we use later \n",
    "import pandas as pd               # we need pandas to import the foot_mouth_original.xls file\n",
    "! pip install xlrd                # apparently we also need xlrd to read the .xls file because pandas is not old school\n",
    "import xlrd                       # le sigh\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize    # importing the word_tokenize function from nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "!pip install autocorrect           # Spellchecker\n",
    "from autocorrect import Speller\n",
    "\n",
    "from nltk.corpus import wordnet                    # Finally, things we need for lemmatising!\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')        # Like a POS-tagger...\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!\n",
    "\n",
    "foot_mouth_df = pd.read_csv ('../code/data/foot_mouth/text.csv')\n",
    "print (foot_mouth_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number         Filename                                    everything_else\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...\n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...\n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...\n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...\n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...\n",
      "Columns renamed :)\n",
      " \n",
      "   Number         Filename                                    everything_else  \\\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
      "\n",
      "  Occupation  \n",
      "0    Group 6  \n",
      "1    Group 6  \n",
      "2    Group 6  \n",
      "3    Group 6  \n",
      "4    Group 5  \n",
      " Occupation Dataframe Created!\n",
      " \n",
      " EVERYTHING READY! :)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "foot_mouth_df = pd.read_csv ('../code/data/foot_mouth/text.csv') \n",
    "\n",
    "foot_mouth_df.columns = [\"Number\", \"Filename\", \"everything_else\"]\n",
    "print(foot_mouth_df.head())\n",
    "\n",
    "print(\"Columns renamed :)\")\n",
    "print(\" \")\n",
    "\n",
    "#Creating New Dataframe with Occupation Column\n",
    "oc_foot_mouth = foot_mouth_df.assign(Occupation = foot_mouth_df['everything_else'].str.extract(r'(\\w+\\s+\\d{1,2})'))\n",
    "\n",
    "print(oc_foot_mouth.head()) # checking if it worked!\n",
    "\n",
    "print(\" Occupation Dataframe Created!\")\n",
    "\n",
    "print(\" \")\n",
    "print(\" EVERYTHING READY! :)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\n",
      "...\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "#Tokenising by word\n",
    "foot_mouth_df['tokenised_words'] = foot_mouth_df.apply(lambda row: nltk.word_tokenize(row['everything_else']), axis=1)\n",
    "\n",
    "#Removing Uppercase\n",
    "foot_mouth_df['txt_lower'] = foot_mouth_df['tokenised_words'].apply(lambda x: [w.lower() for w in x])\n",
    "\n",
    "#Correcting Spelling\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "foot_mouth_df['spell_checked'] = foot_mouth_df['txt_lower'].apply(lambda x: [spell(w) for w in x])\n",
    "\n",
    "def replacement_mapping(x):\n",
    "        if x == \"der\":\n",
    "            return re.sub(\"der\",\"defra\",x)\n",
    "        else:\n",
    "            return x  \n",
    "\n",
    "def replacement_mapping_2(x):\n",
    "        if x == \"ffd\":\n",
    "            return re.sub(\"ffd\",\"fmd\",x)\n",
    "        else:\n",
    "            return x  \n",
    "        \n",
    "foot_mouth_df[\"spell_checked\"] = foot_mouth_df[\"spell_checked\"].apply(lambda x:[replacement_mapping(w) for w in x])\n",
    "foot_mouth_df[\"spell_checked\"] = foot_mouth_df[\"spell_checked\"].apply(lambda x:[replacement_mapping_2(w) for w in x])\n",
    "\n",
    "#Removing Punctuation and getting rid of resulting space\n",
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”-\"      # Define a variable with all the punctuation to remove.\n",
    "print(English_punctuation)                                     # Print that defined variable, just to check it is correct.\n",
    "print(\"...\") \n",
    "\n",
    "\n",
    "def remove_punctuation(from_text):                           # Had to define a function to iterate over the strings in a row\n",
    "    table = str.maketrans('', '', English_punctuation)       # The python function 'maketrans' creates a table that maps\n",
    "    stripped = [w.translate(table) for w in from_text]        # the punctation marks to 'None'. Print the table to check. \n",
    "    return stripped\n",
    "\n",
    "foot_mouth_df['no_punct'] = [remove_punctuation(i) for i in foot_mouth_df['spell_checked']] # Iterating above function to each\n",
    "\n",
    "foot_mouth_df['no_punct_no_space'] = [list(filter(None, sublist)) for sublist in foot_mouth_df['no_punct']]\n",
    "\n",
    "#POS Tagging and Lemmatisation\n",
    "\n",
    "foot_mouth_df['pos_tag'] = foot_mouth_df['no_punct_no_space'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "foot_mouth_df['lemmatised'] = foot_mouth_df['pos_tag'].apply(lambda x: [lemmatizer.lemmatize(y[0], get_wordnet_pos(y[0])) for y in x])\n",
    "\n",
    "#Removing Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "                                                                                                                   \n",
    "new_stopwords =[e for e in stop_words if e not in (\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",'no','not',\"only\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\")]\n",
    "foot_mouth_df['no_stop_words'] = foot_mouth_df['lemmatised'].apply(lambda x: [item for item in x if item not in new_stopwords])                        \n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number         Filename                                    everything_else  \\\n",
      "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
      "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
      "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
      "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
      "\n",
      "                                     tokenised_words  \\\n",
      "0  [Information, about, diarist, Date, of, birth,...   \n",
      "1  [Information, about, diarist, Date, of, birth,...   \n",
      "2  [Information, about, diarist, Date, of, birth,...   \n",
      "3  [Information, about, diarist, Date, of, birth,...   \n",
      "4  [Information, about, diarist, Date, of, birth,...   \n",
      "\n",
      "                                           txt_lower  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                       spell_checked  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                            no_punct  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                   no_punct_no_space  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                             pos_tag  \\\n",
      "0  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "1  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "2  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "3  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "4  [(information, NN), (about, IN), (diarist, NN)...   \n",
      "\n",
      "                                          lemmatised  \\\n",
      "0  [information, about, diarist, date, of, birth,...   \n",
      "1  [information, about, diarist, date, of, birth,...   \n",
      "2  [information, about, diarist, date, of, birth,...   \n",
      "3  [information, about, diarist, date, of, birth,...   \n",
      "4  [information, about, diarist, date, of, birth,...   \n",
      "\n",
      "                                       no_stop_words  \n",
      "0  [information, diarist, date, birth, 1975, gend...  \n",
      "1  [information, diarist, date, birth, 1966, gend...  \n",
      "2  [information, diarist, date, birth, 1964, gend...  \n",
      "3  [information, diarist, date, birth, 1963, gend...  \n",
      "4  [information, diarist, date, birth, 1981, gend...  \n",
      " \n",
      " \n",
      "All Set for Extraction!\n"
     ]
    }
   ],
   "source": [
    "# Appending final processed data column onto the oc_foot_mouth Dataframe\n",
    "\n",
    "processed = foot_mouth_df['no_stop_words']\n",
    "\n",
    "oc_foot_mouth = oc_foot_mouth.join(processed)\n",
    "oc_foot_mouth.rename(columns = {'no_stop_words':'processed_text'}, inplace = True)\n",
    "\n",
    "print(foot_mouth_df.head())\n",
    "print(\" \")\n",
    "oc_foot_mouth.head()\n",
    "\n",
    "print(\" \")\n",
    "print(\"All Set for Extraction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "\n",
       "  Occupation                                     processed_text  \n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...  \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...  \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...  \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...  \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...  \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first (and main) step in the extraction process will be Sentiment Analysis. Sentiment analysis is a commonly used example of automatic classification. To be clear, automatic classification means that a model or learning algorithm has been trained on correctly classified documents and it uses this training to return a probability assessment of what class a new document should belong to. \n",
    "\n",
    "Sentiment analysis works the same way, but usually only has two classes - positive and negative. A trained model looks at new data and says whether that new data is likely to be positive or negative and this is what we will be using to conduct our analysis. Let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by importing and downloading some useful packages, including `textblob`: it is based on `nltk` and has built in sentiment analysis tools. \n",
    "\n",
    "To import the packages, click in the code cell below and hit the 'Run' button at the top of this page or by holding down the 'Shift' key and hitting the 'Enter' key. \n",
    "\n",
    "For the rest of this notebook, I will use 'Run/Shift+Enter' as short hand for 'click in the code cell below and hit the 'Run' button at the top of this page or by hold down the 'Shift' key while hitting the 'Enter' key'. \n",
    "\n",
    "Run/Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk                       # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import csv                        # csv is for importing and working with csv files\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob -q\n",
    "!python -m textblob.download_corpora -q\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse trivial documents with built-in sentiment analysis tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's analyse some simple data before moving on to the more compex foot mouth data.\n",
    "\n",
    "Run/Shift+Enter, as above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc1 = TextBlob(\"Textblob is just super. I love it!\")             # Convert a few basic strings into Textblobs \n",
    "Doc2 = TextBlob(\"Cabbages are the worst. Say no to cabbages!\")    # Textblobs, like other text-mining objects, are often called\n",
    "Doc3 = TextBlob(\"Paris is the capital of France. \")               # 'documents'\n",
    "\n",
    "type(Doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs 1 through 3 are Textblobs, which we can see by the output of type(Doc1). \n",
    "\n",
    "We get a Textblob by passing a string to the function that we imported above. Specifically, this is done by using this format --> Textblob('string goes here'). Textblobs are ready for analysis through the textblob tools, such as the built-in sentiment analysis tool that we see in the code below. \n",
    "\n",
    "Run/Shift+Enter on those Textblobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.47916666666666663, subjectivity=0.6333333333333333)\n",
      "Sentiment(polarity=-1.0, subjectivity=1.0)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(Doc1.sentiment)\n",
    "print(Doc2.sentiment)\n",
    "print(Doc3.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the previous code returns two values for each Textblob object. Polarity refers to a positive-negative spectrum while subjectivity refers to an opinion-fact spectrum. \n",
    "\n",
    "(We can see, for example, that Doc1 is fairly positive but also quite subjective while Doc2 is very negative and very subjective. Doc3, in contrast, is both neutral and factual.)\n",
    "\n",
    "To get only one of the two values, you can call the appropriate sub-function as shown below. \n",
    "\n",
    "Run/Shift+Enter for sub-functional fun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47916666666666663\n",
      "0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(Doc1.sentiment.polarity)\n",
    "print(Doc1.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse foot and mouth dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super. Now let's do this with our foot and mouth Pandas DataFrame. For this we will be looking at the polarity and subjectivity of Occupations 1 and 4 in comparison to the average across occupations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Average Polarity and Subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the overall averages we will be using the following for loop to iterate over all of the rows in our `proccessed_text` column and calculate the overall polarity and subjectivity!\n",
    "\n",
    "Run/Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.07367922973999301, subjectivity=0.4463072592542594)\n"
     ]
    }
   ],
   "source": [
    "for row in oc_foot_mouth:\n",
    "    text = oc_foot_mouth.loc[:,'processed_text'].tolist()\n",
    "    words = \" \".join(str(x) for x in text)\n",
    "    text = TextBlob(words)\n",
    "    total_score = text.sentiment\n",
    "    \n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It appears that sentiments were neutral overall even with them being fairly subjective. This is quite interesting given how sad the topic is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis by Occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do our sentiment analysis on Groups 1 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use the same `==` operator used in the pre-processing stage to filter the dataframe by Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 1']\n",
    "group4_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a quick check to see if the Dataframe has been filtered correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number         Filename  \\\n",
      "33      33  5407diary47.rtf   \n",
      "34      34  5407diary48.rtf   \n",
      "35      35  5407diary49.rtf   \n",
      "36      36  5407diary52.rtf   \n",
      "37      37  5407diary53.rtf   \n",
      "38      38  5407diary54.rtf   \n",
      "40      40     5407fg01.rtf   \n",
      "80      80    5407int47.rtf   \n",
      "81      81    5407int48.rtf   \n",
      "82      82    5407int49.rtf   \n",
      "83      83    5407int52.rtf   \n",
      "84      84    5407int53.rtf   \n",
      "85      85    5407int54.rtf   \n",
      "\n",
      "                                      everything_else Occupation  \\\n",
      "33  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "34  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "35  \\t\\nInformation about diarist\\nDate of birth: ...    Group 1   \n",
      "36  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "37  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "38  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
      "40  \\nGroups Discussion with Members of  Farmers F...    Group 1   \n",
      "80  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "81  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "82  \\nDate of Interview: 22/01/02\\n\\nInformation a...    Group 1   \n",
      "83  \\nDate of Interview: 08/01/02\\n\\nInformation a...    Group 1   \n",
      "84  \\nDate of Interview: 21/01/02\\n\\nInformation a...    Group 1   \n",
      "85  \\nDate of Interview: 17/01/02\\n\\nInformation a...    Group 1   \n",
      "\n",
      "                                       processed_text  \\\n",
      "33  [information, diarist, date, birth, 1956, gend...   \n",
      "34  [information, diarist, date, birth, 1948, gend...   \n",
      "35  [information, diarist, date, birth, 1957, gend...   \n",
      "36  [information, diarist, date, birth, 1969, gend...   \n",
      "37  [information, diarist, date, birth, 1952, gend...   \n",
      "38  [information, diarist, date, birth, 1943, gend...   \n",
      "40  [group, discussion, member, farmer, farmer, wo...   \n",
      "80  [date, interview, 170102, information, panel, ...   \n",
      "81  [date, interview, 170102, information, panel, ...   \n",
      "82  [date, interview, 220102, information, panel, ...   \n",
      "83  [date, interview, 080102, information, panel, ...   \n",
      "84  [date, interview, 210102, information, panel, ...   \n",
      "85  [date, interview, 170102, information, panel, ...   \n",
      "\n",
      "                                             polarity  \n",
      "33  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "34  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "35  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "36  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "37  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "38  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "40  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "80  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "81  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "82  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "83  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "84  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "85  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      " \n",
      "length is 13\n"
     ]
    }
   ],
   "source": [
    "#For group 1\n",
    "print(group1_foot.loc[:])\n",
    "\n",
    "print(\" \")\n",
    "print(\"length is \" + str(len(group1_foot))) # length should be 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number         Filename  \\\n",
      "12      12  5407diary19.rtf   \n",
      "13      13  5407diary21.rtf   \n",
      "14      14  5407diary22.rtf   \n",
      "15      15  5407diary23.rtf   \n",
      "16      16  5407diary24.rtf   \n",
      "17      17  5407diary26.rtf   \n",
      "32      32  5407diary44.rtf   \n",
      "43      43     5407fg04.rtf   \n",
      "58      58    5407int19.rtf   \n",
      "59      59    5407int20.rtf   \n",
      "60      60    5407int21.rtf   \n",
      "61      61    5407int22.rtf   \n",
      "62      62    5407int23.rtf   \n",
      "63      63    5407int24.rtf   \n",
      "64      64    5407int26.rtf   \n",
      "79      79    5407int44.rtf   \n",
      "\n",
      "                                      everything_else Occupation  \\\n",
      "12  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "13  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "14  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "15  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "16  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "17  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "32  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
      "43  \\nNO AUDIO RECORDING\\n\\nGroups Discussion with...    Group 4   \n",
      "58  \\nDate of Interview: 23/02/02\\n\\nInformation a...    Group 4   \n",
      "59  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "60  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "61  \\nDate of Interview: 27/02/02\\n\\nInformation a...    Group 4   \n",
      "62  \\nDate of Interview: 21/02/02\\n\\nInformation a...    Group 4   \n",
      "63  \\nDate of Interview: 21/02/02\\n\\nInformation a...    Group 4   \n",
      "64  \\nDate of Interview: 01/03/02\\n\\nInformation a...    Group 4   \n",
      "79  \\nDate of Interview: 26/02/02\\n\\nInformation a...    Group 4   \n",
      "\n",
      "                                       processed_text  \\\n",
      "12  [information, diarist, date, birth, 1953, gend...   \n",
      "13  [information, diarist, date, birth, 1971, gend...   \n",
      "14  [information, diarist, date, birth, 1961, gend...   \n",
      "15  [information, diarist, date, birth, 1965, gend...   \n",
      "16  [information, diarist, date, birth, 1959, gend...   \n",
      "17  [information, diarist, date, birth, 1971, gend...   \n",
      "32  [information, diarist, date, birth, 1962, gend...   \n",
      "43  [no, audio, record, group, discussion, member,...   \n",
      "58  [date, interview, 230202, information, panel, ...   \n",
      "59  [date, interview, 260202, information, panel, ...   \n",
      "60  [date, interview, 260202, information, panel, ...   \n",
      "61  [date, interview, 270202, information, panel, ...   \n",
      "62  [date, interview, 210202, information, panel, ...   \n",
      "63  [date, interview, 210202, information, panel, ...   \n",
      "64  [date, interview, 010302, information, panel, ...   \n",
      "79  [date, interview, 260202, information, panel, ...   \n",
      "\n",
      "                                             polarity  \n",
      "12  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "13  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "14  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "15  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "16  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "17  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "32  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "43  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "58  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "59  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "60  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "61  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "62  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "63  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "64  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      "79  <generator object <lambda>.<locals>.<genexpr> ...  \n",
      " \n",
      "length is 16\n"
     ]
    }
   ],
   "source": [
    "#For group 4\n",
    "print(group4_foot.loc[:])\n",
    "\n",
    "print(\" \")\n",
    "print(\"length is \" + str(len(group4_foot))) # length should be 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now let's do Sentiment analysis on these new filtered Dataframes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group1_foot.loc[:,'processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Writing the codes for the analysis\n",
    "# Group 1\n",
    "for row in group1_foot:\n",
    "    g1_text = group1_foot.loc[:,'processed_text'].tolist()\n",
    "    words = \" \".join(str(x) for x)\n",
    "    g1_text = TextBlob(words)\n",
    "    group1_score = g1_text.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Group 4\n",
    "for row in group4_foot:\n",
    "    g4_text = group4_foot.loc[:,'processed_text'].tolist()\n",
    "    g4_words = \" \".join(str(x) for x in text)\n",
    "    g4_text = TextBlob(g4_words)\n",
    "    group4_score = g4_text.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for Group 1 are Sentiment(polarity=0.0, subjectivity=0.0)\n",
      " \n",
      "The scores for Group 4 are Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Finding out the scores for each\n",
    "\n",
    "print(\"The scores for Group 1 are \" + str(group1_score))\n",
    "print(\" \")\n",
    "print(\"The scores for Group 4 are \" + str(group4_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that's strange... Will have to look more closely to see what the issue is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting out Sentiment Score Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Sentiment and Polarity by Row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the following code to have a closer look at the sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth['scores'] = oc_foot_mouth['processed_text'].apply(lambda x: [TextBlob(y).sentiment for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                              scores  \n",
       "0  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "1  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "2  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "3  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "4  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "5  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "6  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "7  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "8  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  \n",
       "9  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.25, subjectivity=0.25),\n",
       " Sentiment(polarity=-0.05, subjectivity=0.4),\n",
       " Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.14285714285714285),\n",
       " Sentiment(polarity=-0.1, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.8, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.1875, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.3333333333333333, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.3),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.3, subjectivity=0.6),\n",
       " Sentiment(polarity=-0.8, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.3, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.3, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.4000000000000001, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.06666666666666667),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.3333333333333333, subjectivity=0.8333333333333334),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.25, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.16666666666666666, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.35, subjectivity=0.55),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.35, subjectivity=0.55),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.13636363636363635, subjectivity=0.45454545454545453),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.13636363636363635, subjectivity=0.45454545454545453),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.10000000000000002, subjectivity=0.3833333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.25, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.3666666666666667, subjectivity=0.7000000000000001),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.9),\n",
       " Sentiment(polarity=0.43333333333333335, subjectivity=0.8333333333333334),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-1.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2857142857142857, subjectivity=0.5357142857142857),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.1),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.8, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.06666666666666667),\n",
       " Sentiment(polarity=-0.05, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.9, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.3, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.8888888888888888),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.13636363636363635, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.06666666666666667),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.3),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.16666666666666666, subjectivity=0.16666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.6),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.16666666666666666, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.25, subjectivity=0.25),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.1),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.3),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.35, subjectivity=0.55),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.25, subjectivity=0.25),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.8, subjectivity=1.0),\n",
       " Sentiment(polarity=0.13636363636363635, subjectivity=0.45454545454545453),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.1),\n",
       " Sentiment(polarity=0.13636363636363635, subjectivity=0.45454545454545453),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.3333333333333333, subjectivity=0.8333333333333334),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.25, subjectivity=0.25),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.6, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.3, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.8, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.65),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.2, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.2, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.8888888888888888),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.06666666666666667),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.8, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.2, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-1.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.1875, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.06666666666666667),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.175, subjectivity=0.175),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.2, subjectivity=0.6),\n",
       " Sentiment(polarity=0.0, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.625),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.1875, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.05, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.4, subjectivity=0.7),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.2, subjectivity=0.5),\n",
       " Sentiment(polarity=-0.7142857142857143, subjectivity=0.8571428571428571),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.6, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=1.0, subjectivity=0.3),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.3333333333333333, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.1),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.8, subjectivity=0.9),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.625),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.4, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.625),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.25, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=1.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.125, subjectivity=0.375),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.4),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.25, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.25, subjectivity=0.3333333333333333),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.5),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=-0.05, subjectivity=0.15),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.1, subjectivity=0.2),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.5, subjectivity=0.75),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.05, subjectivity=0.1),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " Sentiment(polarity=0.0, subjectivity=0.0),\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth.loc[1,'scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it looks like there is an issue with there being too many neutral words in this dataset that are skewing the results (for example whole phrases such as \" Information about diarist\" or \"date of birth\" that are just documentations and do not have anything to do with the actual responses submitted by the participants). So we will need to filter out all of these neutral scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Sentiment Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we will be splitting the sentiment scores by polarity and subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth['polarity'] = oc_foot_mouth['processed_text'].apply(lambda x: [TextBlob(y).sentiment.polarity for y in x])\n",
    "oc_foot_mouth['subjectivity'] = oc_foot_mouth['processed_text'].apply(lambda x: [TextBlob(y).sentiment.subjectivity for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " -0.05,\n",
       " -0.2916666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1,\n",
       " 0.0,\n",
       " -0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1875,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " -0.3,\n",
       " -0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4000000000000001,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.35,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.35,\n",
       " 0.0,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10000000000000002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3666666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.43333333333333335,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " -0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2916666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.35,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13636363636363635,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " -0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.175,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.1875,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.2,\n",
       " -0.7142857142857143,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.6999999999999998,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.125,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " -0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking this worked\n",
    "\n",
    "oc_foot_mouth.loc[1,'polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.4,\n",
       " 0.5416666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14285714285714285,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8333333333333334,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.55,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.55,\n",
       " 0.0,\n",
       " 0.45454545454545453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.45454545454545453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3833333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7000000000000001,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.8333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5357142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8888888888888888,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5416666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.55,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.45454545454545453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.45454545454545453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8333333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.65,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8888888888888888,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.175,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.625,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.8571428571428571,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6000000000000001,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.3,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.9,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.625,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.625,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.375,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth.loc[1,'subjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth['pol_filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['polarity']]\n",
    "oc_foot_mouth['subj_filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>scores</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>pol_filtered</th>\n",
       "      <th>subj_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.3, 0.7, 0.1, -0.16666666666666666, ...</td>\n",
       "      <td>[0.25, 0.2, 0.06666666666666667, 0.4, 0.066666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.05, -0.2916666666666667, -0.1, -0.8...</td>\n",
       "      <td>[0.25, 0.4, 0.5416666666666666, 0.142857142857...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.13636363636363635, -0.16666666666666666, 0....</td>\n",
       "      <td>[0.45454545454545453, 0.16666666666666666, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, 1.0, 1.0, 0.7, 0.2, -0.6999999999999998,...</td>\n",
       "      <td>[0.2, 0.3, 0.3, 0.6000000000000001, 0.2, 0.666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.6, 0.5, -0.25, 0.2, 0.25, 0.25, -0.25, 0.1...</td>\n",
       "      <td>[0.9, 0.4, 0.5, 0.25, 0.2, 0.25, 0.25, 1.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, -0.5, 0.5, -0.6, 0.8, 0.1, 0.5, -0.6, 0....</td>\n",
       "      <td>[0.4, 1.0, 0.06666666666666667, 1.0, 1.0, 0.75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.5, -0.25, 0.375, 0.5, 0.05000000000000002,...</td>\n",
       "      <td>[1.0, 1.0, 0.06666666666666667, 0.25, 0.75, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4, 0.10000000000000002, 0.15, -0.6999999999...</td>\n",
       "      <td>[0.7, 1.0, 0.3833333333333333, 0.6499999999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.7, -0.125, 0.7, 0.3333333333333333, 0.7, 0....</td>\n",
       "      <td>[0.9, 0.3333333333333333, 1.0, 0.6000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4, 0.2, 0.25, 0.25, 0.2, 0.7, 0.36666666666...</td>\n",
       "      <td>[0.06666666666666667, 0.5, 0.4, 0.066666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5407diary17.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1936...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1936, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.25, -0.225, 0.7, 0.7, -0.25, -0.69999999999...</td>\n",
       "      <td>[0.3333333333333333, 0.3, 0.6000000000000001, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5407diary18.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1954, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, -0.5, -0.4, 0.25, 0.1, 0.2, 0.7, 0.0625,...</td>\n",
       "      <td>[0.4, 0.5, 0.6666666666666666, 0.7, 0.33333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>5407diary19.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1953, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.13333333333333333, -0.5, 0.1, 0.6, 0.6, 0....</td>\n",
       "      <td>[0.16666666666666666, 1.0, 0.3, 1.0, 1.0, 0.3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>5407diary21.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1971, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.25, 0.2, 0.2, -0.8, 0.8, 0.2, 0.13636363636...</td>\n",
       "      <td>[0.3333333333333333, 0.2, 0.2, 0.8, 0.9, 0.2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>5407diary22.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1961, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.7, -0.25, -0.05, -0.1, -0.13333333333333333...</td>\n",
       "      <td>[0.6000000000000001, 0.25, 0.15, 0.06666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>5407diary23.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1965, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.1875, 0.5, 0.16666666666666666, 0.4, 0.136...</td>\n",
       "      <td>[0.5, 0.5, 0.5, 0.3333333333333333, 1.0, 0.454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5407diary24.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1959, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.25, 0.8, 0.1, 0.1, -0.5, -0.316666666666666...</td>\n",
       "      <td>[0.25, 0.3333333333333333, 0.75, 0.6, 0.4, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>5407diary26.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 4</td>\n",
       "      <td>[information, diarist, date, birth, 1971, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, 0.7, 0.1, 0.2, -0.2, 0.2, 0.1, -0.1, -0....</td>\n",
       "      <td>[0.3, 0.6000000000000001, 0.3, 0.2, 0.4, 0.2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>5407diary27.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4, 0.4, 0.2857142857142857, -0.3, -0.6, 0.2...</td>\n",
       "      <td>[0.6, 0.5, 0.5, 0.5357142857142857, 0.4, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>5407diary28.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 3</td>\n",
       "      <td>[information, diarist, date, birth, 1968, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.2, 0.1, 0.13636363636363635, 0.7, 0.2, 0.5...</td>\n",
       "      <td>[0.4, 0.3, 0.45454545454545453, 0.600000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number         Filename  \\\n",
       "0        0  5407diary02.rtf   \n",
       "1        1  5407diary03.rtf   \n",
       "2        2  5407diary07.rtf   \n",
       "3        3  5407diary08.rtf   \n",
       "4        4  5407diary09.rtf   \n",
       "5        5  5407diary10.rtf   \n",
       "6        6  5407diary13.rtf   \n",
       "7        7  5407diary14.rtf   \n",
       "8        8  5407diary15.rtf   \n",
       "9        9  5407diary16.rtf   \n",
       "10      10  5407diary17.rtf   \n",
       "11      11  5407diary18.rtf   \n",
       "12      12  5407diary19.rtf   \n",
       "13      13  5407diary21.rtf   \n",
       "14      14  5407diary22.rtf   \n",
       "15      15  5407diary23.rtf   \n",
       "16      16  5407diary24.rtf   \n",
       "17      17  5407diary26.rtf   \n",
       "18      18  5407diary27.rtf   \n",
       "19      19  5407diary28.rtf   \n",
       "\n",
       "                                      everything_else Occupation  \\\n",
       "0   \\n\\nInformation about diarist\\nDate of birth: ...    Group 6   \n",
       "1   Information about diarist\\nDate of birth: 1966...    Group 6   \n",
       "2   \\n\\nInformation about diarist\\nDate of birth: ...    Group 6   \n",
       "3   Information about diarist\\nDate of birth: 1963...    Group 6   \n",
       "4   Information about diarist\\nDate of birth: 1981...    Group 5   \n",
       "5   Information about diarist\\nDate of birth: 1937...    Group 5   \n",
       "6   Information about diarist\\nDate of birth: 1947...    Group 5   \n",
       "7   \\nInformation about diarist\\nDate of birth: 19...    Group 5   \n",
       "8   Information about diarist\\nDate of birth: 1949...    Group 5   \n",
       "9   \\nInformation about diarist\\nDate of birth: 19...    Group 5   \n",
       "10  Information about diarist\\nDate of birth: 1936...    Group 5   \n",
       "11  \\nInformation about diarist\\nDate of birth: 19...    Group 5   \n",
       "12  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "13  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "14  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "15  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "16  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "17  \\nInformation about diarist\\nDate of birth: 19...    Group 4   \n",
       "18  \\nInformation about diarist\\nDate of birth: 19...    Group 3   \n",
       "19  \\nInformation about diarist\\nDate of birth: 19...    Group 3   \n",
       "\n",
       "                                       processed_text  \\\n",
       "0   [information, diarist, date, birth, 1975, gend...   \n",
       "1   [information, diarist, date, birth, 1966, gend...   \n",
       "2   [information, diarist, date, birth, 1964, gend...   \n",
       "3   [information, diarist, date, birth, 1963, gend...   \n",
       "4   [information, diarist, date, birth, 1981, gend...   \n",
       "5   [information, diarist, date, birth, 1937, gend...   \n",
       "6   [information, diarist, date, birth, 1947, gend...   \n",
       "7   [information, diarist, date, birth, 1964, gend...   \n",
       "8   [information, diarist, date, birth, 1949, gend...   \n",
       "9   [information, diarist, date, birth, 1951, gend...   \n",
       "10  [information, diarist, date, birth, 1936, gend...   \n",
       "11  [information, diarist, date, birth, 1954, gend...   \n",
       "12  [information, diarist, date, birth, 1953, gend...   \n",
       "13  [information, diarist, date, birth, 1971, gend...   \n",
       "14  [information, diarist, date, birth, 1961, gend...   \n",
       "15  [information, diarist, date, birth, 1965, gend...   \n",
       "16  [information, diarist, date, birth, 1959, gend...   \n",
       "17  [information, diarist, date, birth, 1971, gend...   \n",
       "18  [information, diarist, date, birth, 1951, gend...   \n",
       "19  [information, diarist, date, birth, 1968, gend...   \n",
       "\n",
       "                                             polarity  \\\n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "13  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "14  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "15  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "16  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               scores  \\\n",
       "0   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "1   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "2   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "3   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "4   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "5   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "6   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "7   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "8   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "9   [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "10  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "11  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "12  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "13  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "14  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "15  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "16  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "17  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "18  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "19  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "\n",
       "                                         subjectivity  \\\n",
       "0   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "13  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "14  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "15  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "16  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         pol_filtered  \\\n",
       "0   [-0.25, -0.3, 0.7, 0.1, -0.16666666666666666, ...   \n",
       "1   [-0.25, -0.05, -0.2916666666666667, -0.1, -0.8...   \n",
       "2   [0.13636363636363635, -0.16666666666666666, 0....   \n",
       "3   [0.1, 1.0, 1.0, 0.7, 0.2, -0.6999999999999998,...   \n",
       "4   [-0.6, 0.5, -0.25, 0.2, 0.25, 0.25, -0.25, 0.1...   \n",
       "5   [0.1, -0.5, 0.5, -0.6, 0.8, 0.1, 0.5, -0.6, 0....   \n",
       "6   [-0.5, -0.25, 0.375, 0.5, 0.05000000000000002,...   \n",
       "7   [0.4, 0.10000000000000002, 0.15, -0.6999999999...   \n",
       "8   [0.7, -0.125, 0.7, 0.3333333333333333, 0.7, 0....   \n",
       "9   [0.4, 0.2, 0.25, 0.25, 0.2, 0.7, 0.36666666666...   \n",
       "10  [0.25, -0.225, 0.7, 0.7, -0.25, -0.69999999999...   \n",
       "11  [0.1, -0.5, -0.4, 0.25, 0.1, 0.2, 0.7, 0.0625,...   \n",
       "12  [-0.13333333333333333, -0.5, 0.1, 0.6, 0.6, 0....   \n",
       "13  [0.25, 0.2, 0.2, -0.8, 0.8, 0.2, 0.13636363636...   \n",
       "14  [0.7, -0.25, -0.05, -0.1, -0.13333333333333333...   \n",
       "15  [-0.1875, 0.5, 0.16666666666666666, 0.4, 0.136...   \n",
       "16  [0.25, 0.8, 0.1, 0.1, -0.5, -0.316666666666666...   \n",
       "17  [0.1, 0.7, 0.1, 0.2, -0.2, 0.2, 0.1, -0.1, -0....   \n",
       "18  [0.4, 0.4, 0.2857142857142857, -0.3, -0.6, 0.2...   \n",
       "19  [-0.2, 0.1, 0.13636363636363635, 0.7, 0.2, 0.5...   \n",
       "\n",
       "                                        subj_filtered  \n",
       "0   [0.25, 0.2, 0.06666666666666667, 0.4, 0.066666...  \n",
       "1   [0.25, 0.4, 0.5416666666666666, 0.142857142857...  \n",
       "2   [0.45454545454545453, 0.16666666666666666, 0.6...  \n",
       "3   [0.2, 0.3, 0.3, 0.6000000000000001, 0.2, 0.666...  \n",
       "4   [0.9, 0.4, 0.5, 0.25, 0.2, 0.25, 0.25, 1.0, 0....  \n",
       "5   [0.4, 1.0, 0.06666666666666667, 1.0, 1.0, 0.75...  \n",
       "6   [1.0, 1.0, 0.06666666666666667, 0.25, 0.75, 0....  \n",
       "7   [0.7, 1.0, 0.3833333333333333, 0.6499999999999...  \n",
       "8   [0.9, 0.3333333333333333, 1.0, 0.6000000000000...  \n",
       "9   [0.06666666666666667, 0.5, 0.4, 0.066666666666...  \n",
       "10  [0.3333333333333333, 0.3, 0.6000000000000001, ...  \n",
       "11  [0.4, 0.5, 0.6666666666666666, 0.7, 0.33333333...  \n",
       "12  [0.16666666666666666, 1.0, 0.3, 1.0, 1.0, 0.3,...  \n",
       "13  [0.3333333333333333, 0.2, 0.2, 0.8, 0.9, 0.2, ...  \n",
       "14  [0.6000000000000001, 0.25, 0.15, 0.06666666666...  \n",
       "15  [0.5, 0.5, 0.5, 0.3333333333333333, 1.0, 0.454...  \n",
       "16  [0.25, 0.3333333333333333, 0.75, 0.6, 0.4, 1.0...  \n",
       "17  [0.3, 0.6000000000000001, 0.3, 0.2, 0.4, 0.2, ...  \n",
       "18  [0.6, 0.5, 0.5, 0.5357142857142857, 0.4, 1.0, ...  \n",
       "19  [0.4, 0.3, 0.45454545454545453, 0.600000000000...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much better! Now lets do the analysis on these new scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Filtered Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will just be calculating the means of the scores! First though we will need to import numpy so we can use its mean function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be creating new columns for the means of each polarity and subjectivity row - this will make thngs easier for the next steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>scores</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>pol_filtered</th>\n",
       "      <th>subj_filtered</th>\n",
       "      <th>pol_mean</th>\n",
       "      <th>subj_mean</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.3, 0.7, 0.1, -0.16666666666666666, ...</td>\n",
       "      <td>[0.25, 0.2, 0.06666666666666667, 0.4, 0.066666...</td>\n",
       "      <td>0.127907</td>\n",
       "      <td>0.479233</td>\n",
       "      <td>0.127907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.25, -0.05, -0.2916666666666667, -0.1, -0.8...</td>\n",
       "      <td>[0.25, 0.4, 0.5416666666666666, 0.142857142857...</td>\n",
       "      <td>0.083887</td>\n",
       "      <td>0.514394</td>\n",
       "      <td>0.083887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.13636363636363635, -0.16666666666666666, 0....</td>\n",
       "      <td>[0.45454545454545453, 0.16666666666666666, 0.6...</td>\n",
       "      <td>0.202948</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.202948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, 1.0, 1.0, 0.7, 0.2, -0.6999999999999998,...</td>\n",
       "      <td>[0.2, 0.3, 0.3, 0.6000000000000001, 0.2, 0.666...</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.505286</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.6, 0.5, -0.25, 0.2, 0.25, 0.25, -0.25, 0.1...</td>\n",
       "      <td>[0.9, 0.4, 0.5, 0.25, 0.2, 0.25, 0.25, 1.0, 0....</td>\n",
       "      <td>0.159727</td>\n",
       "      <td>0.521339</td>\n",
       "      <td>0.159727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.1, -0.5, 0.5, -0.6, 0.8, 0.1, 0.5, -0.6, 0....</td>\n",
       "      <td>[0.4, 1.0, 0.06666666666666667, 1.0, 1.0, 0.75...</td>\n",
       "      <td>0.108185</td>\n",
       "      <td>0.483870</td>\n",
       "      <td>0.108185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.5, -0.25, 0.375, 0.5, 0.05000000000000002,...</td>\n",
       "      <td>[1.0, 1.0, 0.06666666666666667, 0.25, 0.75, 0....</td>\n",
       "      <td>0.152179</td>\n",
       "      <td>0.500282</td>\n",
       "      <td>0.152179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4, 0.10000000000000002, 0.15, -0.6999999999...</td>\n",
       "      <td>[0.7, 1.0, 0.3833333333333333, 0.6499999999999...</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>0.533937</td>\n",
       "      <td>0.139130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.7, -0.125, 0.7, 0.3333333333333333, 0.7, 0....</td>\n",
       "      <td>[0.9, 0.3333333333333333, 1.0, 0.6000000000000...</td>\n",
       "      <td>0.182986</td>\n",
       "      <td>0.556282</td>\n",
       "      <td>0.182986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.4, 0.2, 0.25, 0.25, 0.2, 0.7, 0.36666666666...</td>\n",
       "      <td>[0.06666666666666667, 0.5, 0.4, 0.066666666666...</td>\n",
       "      <td>0.148523</td>\n",
       "      <td>0.508256</td>\n",
       "      <td>0.148523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                            polarity  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                              scores  \\\n",
       "0  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "1  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "2  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "3  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "4  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "5  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "6  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "7  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "8  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "9  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "\n",
       "                                        subjectivity  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                        pol_filtered  \\\n",
       "0  [-0.25, -0.3, 0.7, 0.1, -0.16666666666666666, ...   \n",
       "1  [-0.25, -0.05, -0.2916666666666667, -0.1, -0.8...   \n",
       "2  [0.13636363636363635, -0.16666666666666666, 0....   \n",
       "3  [0.1, 1.0, 1.0, 0.7, 0.2, -0.6999999999999998,...   \n",
       "4  [-0.6, 0.5, -0.25, 0.2, 0.25, 0.25, -0.25, 0.1...   \n",
       "5  [0.1, -0.5, 0.5, -0.6, 0.8, 0.1, 0.5, -0.6, 0....   \n",
       "6  [-0.5, -0.25, 0.375, 0.5, 0.05000000000000002,...   \n",
       "7  [0.4, 0.10000000000000002, 0.15, -0.6999999999...   \n",
       "8  [0.7, -0.125, 0.7, 0.3333333333333333, 0.7, 0....   \n",
       "9  [0.4, 0.2, 0.25, 0.25, 0.2, 0.7, 0.36666666666...   \n",
       "\n",
       "                                       subj_filtered  pol_mean  subj_mean  \\\n",
       "0  [0.25, 0.2, 0.06666666666666667, 0.4, 0.066666...  0.127907   0.479233   \n",
       "1  [0.25, 0.4, 0.5416666666666666, 0.142857142857...  0.083887   0.514394   \n",
       "2  [0.45454545454545453, 0.16666666666666666, 0.6...  0.202948   0.517709   \n",
       "3  [0.2, 0.3, 0.3, 0.6000000000000001, 0.2, 0.666...  0.126500   0.505286   \n",
       "4  [0.9, 0.4, 0.5, 0.25, 0.2, 0.25, 0.25, 1.0, 0....  0.159727   0.521339   \n",
       "5  [0.4, 1.0, 0.06666666666666667, 1.0, 1.0, 0.75...  0.108185   0.483870   \n",
       "6  [1.0, 1.0, 0.06666666666666667, 0.25, 0.75, 0....  0.152179   0.500282   \n",
       "7  [0.7, 1.0, 0.3833333333333333, 0.6499999999999...  0.139130   0.533937   \n",
       "8  [0.9, 0.3333333333333333, 1.0, 0.6000000000000...  0.182986   0.556282   \n",
       "9  [0.06666666666666667, 0.5, 0.4, 0.066666666666...  0.148523   0.508256   \n",
       "\n",
       "       Mean  \n",
       "0  0.127907  \n",
       "1  0.083887  \n",
       "2  0.202948  \n",
       "3  0.126500  \n",
       "4  0.159727  \n",
       "5  0.108185  \n",
       "6  0.152179  \n",
       "7  0.139130  \n",
       "8  0.182986  \n",
       "9  0.148523  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth['pol_mean'] = oc_foot_mouth['pol_filtered'].apply(np.mean)\n",
    "oc_foot_mouth['subj_mean'] = oc_foot_mouth['subj_filtered'].apply(np.mean)\n",
    "\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now lets work go back to our original analyses of the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Score Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09797969069667002\n",
      "0.5004429816411067\n"
     ]
    }
   ],
   "source": [
    "print(oc_foot_mouth[\"pol_mean\"].mean())\n",
    "print(oc_foot_mouth[\"subj_mean\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay s compared to before both the polarity AND the subjectivity are slightly higher, but still not significantly different to what was found before. Let's see if this translates to any difference per occupation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation Sentiment Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>scores</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>pol_filtered</th>\n",
       "      <th>subj_filtered</th>\n",
       "      <th>pol_mean</th>\n",
       "      <th>subj_mean</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>5407diary47.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>[information, diarist, date, birth, 1956, gend...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.25, 0.8, 0.25, 0.3666666666666667, 0.400000...</td>\n",
       "      <td>[0.3333333333333333, 1.0, 0.3333333333333333, ...</td>\n",
       "      <td>0.083072</td>\n",
       "      <td>0.468844</td>\n",
       "      <td>0.083072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number         Filename  \\\n",
       "33      33  5407diary47.rtf   \n",
       "\n",
       "                                      everything_else Occupation  \\\n",
       "33  \\nInformation about diarist\\nDate of birth: 19...    Group 1   \n",
       "\n",
       "                                       processed_text  \\\n",
       "33  [information, diarist, date, birth, 1956, gend...   \n",
       "\n",
       "                                             polarity  \\\n",
       "33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                               scores  \\\n",
       "33  [(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0...   \n",
       "\n",
       "                                         subjectivity  \\\n",
       "33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         pol_filtered  \\\n",
       "33  [0.25, 0.8, 0.25, 0.3666666666666667, 0.400000...   \n",
       "\n",
       "                                        subj_filtered  pol_mean  subj_mean  \\\n",
       "33  [0.3333333333333333, 1.0, 0.3333333333333333, ...  0.083072   0.468844   \n",
       "\n",
       "        Mean  \n",
       "33  0.083072  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group1_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 1']\n",
    "group4_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08583683066634482\n",
      "0.4784183401572556\n",
      " \n",
      "0.10520366263253528\n",
      "0.5051201291424214\n"
     ]
    }
   ],
   "source": [
    "print(group1_foot[\"pol_mean\"].mean())\n",
    "print(group1_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(group4_foot[\"pol_mean\"].mean())\n",
    "print(group4_foot[\"subj_mean\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok much better than before! Although it does definitely look like things are not that much diifferet to the general average!\n",
    "\n",
    "Now for visualisations purposes I will get the averages for the other occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 2']\n",
    "group3_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 3']\n",
    "group5_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 5']\n",
    "group6_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1062511833977345\n",
      "0.5086427142605836\n",
      " \n",
      "0.07805426313312835\n",
      "0.4975191717791073\n",
      " \n",
      "0.11024942120003758\n",
      "0.5115891299348682\n",
      " \n",
      "0.10246320328808936\n",
      "0.4940893600064796\n"
     ]
    }
   ],
   "source": [
    "print(group2_foot[\"pol_mean\"].mean())\n",
    "print(group2_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(group3_foot[\"pol_mean\"].mean())\n",
    "print(group3_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(group5_foot[\"pol_mean\"].mean())\n",
    "print(group5_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(group6_foot[\"pol_mean\"].mean())\n",
    "print(group6_foot[\"subj_mean\"].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textblob on Different dataset formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the \"analysis per word\" method gives strange results we will try doing it on less processed bits of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis On Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>0.102367</td>\n",
       "      <td>0.430962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>0.071534</td>\n",
       "      <td>0.483925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>0.161850</td>\n",
       "      <td>0.479241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>0.094893</td>\n",
       "      <td>0.446693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>0.120849</td>\n",
       "      <td>0.486689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "\n",
       "  Occupation                                     processed_text  polarity  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...  0.102367   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...  0.071534   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...  0.161850   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...  0.094893   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...  0.120849   \n",
       "\n",
       "   subjectivity  \n",
       "0      0.430962  \n",
       "1      0.483925  \n",
       "2      0.479241  \n",
       "3      0.446693  \n",
       "4      0.486689  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth[['polarity', 'subjectivity']] = oc_foot_mouth['everything_else'].apply(lambda text: pd.Series(TextBlob(text).sentiment))\n",
    "oc_foot_mouth[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 1']\n",
    "group4_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 4']\n",
    "group2_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 2']\n",
    "group3_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 3']\n",
    "group5_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 5']\n",
    "group6_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score averages are: \n",
      "0.07005421874423302\n",
      "0.44396826245946436\n",
      " \n",
      " Group 1 averages are: \n",
      "0.06223521503453617\n",
      "0.42215742338412193\n",
      " \n",
      " Group 4 averages are: \n",
      "0.07092672635674173\n",
      "0.4398351067769926\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.07292686149497901\n",
      "0.4542654403661014\n",
      " \n",
      "Group3:\n",
      "0.06097999793431373\n",
      "0.44443789720562005\n",
      " \n",
      "Group5:\n",
      "0.07731680608744904\n",
      "0.4521710803801739\n",
      " \n",
      "Group6:\n",
      "0.07745590373464921\n",
      "0.44974279078552926\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth[\"polarity\"].mean())\n",
    "print(oc_foot_mouth[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot[\"polarity\"].mean())\n",
    "print(group1_foot[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot[\"polarity\"].mean())\n",
    "print(group4_foot[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot[\"polarity\"].mean())\n",
    "print(group2_foot[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot[\"polarity\"].mean())\n",
    "print(group3_foot[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot[\"polarity\"].mean())\n",
    "print(group5_foot[\"subjectivity\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot[\"polarity\"].mean())\n",
    "print(group6_foot[\"subjectivity\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay looks mostly the same! Let's try by sentence! But first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "\n",
       "  Occupation                                     processed_text  \n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...  \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...  \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...  \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...  \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth.drop('polarity', axis=1, inplace=True)\n",
    "oc_foot_mouth.drop('subjectivity', axis=1, inplace=True)\n",
    "oc_foot_mouth.drop('scores', axis=1, inplace=True)\n",
    "\n",
    "oc_foot_mouth[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Information about diarist\\nDate of birth: 1937\\nGender: M\\nOccupation: Group 5\\nGeographic region: North Cumbria\\n\\nWeek 1\\nMonday 11th March 2002\\nWhilst watching the local TV news at 6 p.m. there was a news item that caused us to reflect back on the events a year ago.',\n",
       " 'A young lady had just left a court where she had been found guilty of assaulting a Police Officer and also being in change of an offensive weapon -–a knife.',\n",
       " 'The judge had acquitted her of the offences, he showed leniency towards her.',\n",
       " 'Last year during the FMD crisis she had returned to her home to find that her pet goat had been killed by slaughterers because the animal was within the 3 km radius.',\n",
       " 'She had gone berserk over this and threatened the Police Officer and others with the knife.',\n",
       " 'She had to be forcibly restrained, she was very distraught over this killing.',\n",
       " 'Even after she had appeared in court and had been acquitted of all charges she showed great emotion not only being freed but also quite upset over the loss of the goat.',\n",
       " 'Perhaps her actions didn’t happen to a lot of other people who had similar things happen to them.',\n",
       " 'However, the loss of a lot of pet animals and in some cases, needless slaughter of many farm animals still creates unhappy memories of 2001.',\n",
       " 'Week 2\\nTuesday\\nWhilst walking the dog I met a farmer from the edge of the village who has friends and stock in close proximity to the 2 land fill sites.',\n",
       " 'He is still very concerned about materials on these sites.',\n",
       " 'The nearest site contained hundreds of carcasses.',\n",
       " 'This has been completed and capped.',\n",
       " 'He is concerned about leachate from this site and feels that it doesn’t matter how much clay and soil were used to contain this site, the effects of heavy rain is bound to find a way down and also to drain it.',\n",
       " 'He doesn’t want to plough these fields, nor can he sell stock that have grazed the same fields.',\n",
       " 'There is pyre ash being tipped on the other site.',\n",
       " 'Again, what happens to the rainwater that runs off this site?',\n",
       " 'Also there are concerns about the large flocks of seagulls that visit both sites daily.',\n",
       " 'Another concern is what is happening to the open-cast coal site that is situated almost due south of Gilgarran Village.',\n",
       " 'The farmer I talked to today is concerned about this huge site.',\n",
       " 'No coal has been moved from this site for months.',\n",
       " 'There are concerns that this site is going to be filled with waste.',\n",
       " 'Will it be from FMD sites?',\n",
       " 'We, as a village, are very concerned about rumours of land fill on a huge scale.',\n",
       " 'Friday\\nNoticed that there was work being carried out on the top of the burial site.',\n",
       " 'No villagers have commented on this, despite large yellow diggers operating.',\n",
       " 'Sunday\\nWork continuing on the burial site.',\n",
       " 'Cannot make out what kind of work is being done there\\n\\n\\nWeek 3\\nMonday\\nWork is still going on at the burial site.',\n",
       " 'I still don’t know what is going on, but the diggers involved are the same as when animals were being buried there.',\n",
       " 'When animals were being buried there last year, the smell coming from that site was terrible to say the least!',\n",
       " 'It was not coming from the dead animals as most observers thought, but from decomposing waste material that had already been buried on the site prior to FMD.',\n",
       " 'When excavators dug into the soil to make trenches for the dead animals, they dug into this decomposing matter, hence the terrible smell.',\n",
       " 'Despite the work that is going on there today, no comments from villagers are forthcoming.',\n",
       " 'It seems to me that now that FMD has gone, the general public are not interested any more, unless they read something in the local papers written by some enterprising reporter!',\n",
       " 'Week 4\\nTuesday\\nWork is still going on in the former burial site.',\n",
       " 'Villagers don’t seem to be bothered.',\n",
       " 'FMD is gone, so nobody is interested any more.',\n",
       " 'Wednesday\\nWhilst trying to gain comments from villagers over the effects of FMD, one or two comments from some individuals show concern about the outbreak last year, but don’t seem too concerned over any after effects, if any!',\n",
       " 'Two interesting comments suggest that (1) the outbreak was started deliberately by ‘this country’ in collusion with the agriculturists of the E.E.C so as to concentrate meat production in Europe and leave the UK to concentrate on arable farming; (2) The outbreak was started by a terrorist attack.',\n",
       " 'The Government would not declare this because it would cause widespread panic.',\n",
       " 'Thursday 23:25 hours\\nHuge fire at the site where pyre ash is being tipped.',\n",
       " '250,000 used tyres caught fire.',\n",
       " 'Arson is suspected.',\n",
       " 'Fire fighters tried to contain the blaze, but couldn’t use large amounts of water in case water courses became contaminated.',\n",
       " 'Friday 05:00\\nFire still blazing at the pyre ash site.',\n",
       " 'Later in the morning the fire was showing signs of dying down, apparently it was left to burn itself out.',\n",
       " 'Much heavy smoke pollution was evident, drifting south west for about nine miles.',\n",
       " 'Reading the local evening paper about the blaze, there was also a report that villagers from Disington (1¾ miles from Gilgarran) were complaining of the foul smell from both waste sites.',\n",
       " 'Parish councillors are very concerned about this.',\n",
       " 'Does it coincide with work currently being carried out on the burial site?',\n",
       " 'The smell from these sites plus the fact that animals were buried on one site and pyre ash plus the huge fire from the other site all happening this week is causing concern in this area.',\n",
       " 'But once this ‘hue and cry’ dies down, people will soon forget about it all.',\n",
       " 'Week 5\\nMonday through to Friday, observed work on top of the burial site.',\n",
       " 'Don’t know if any work is still going on on the northern and western sides.',\n",
       " 'Friday\\nLocal weekly paper carried the report on the recent large fire that occurred on the Alco site last week when 250,000 tyres caught fire somehow.',\n",
       " 'It was intersting to read that the fire brigade did not use any water to extinguish the blaze in case pollution occurred in water courses.',\n",
       " 'The fire was left to burn itself out.',\n",
       " 'Saturday\\nBurial site – it looks like there is new soil being tipped on top for some reason.',\n",
       " 'No reported comments froim the Parish Council over this, despite very vociferous objections by them over the use of this and the Alco site in the past.',\n",
       " 'Sunday\\nTalked to our local County Councillor (who lives in this village).',\n",
       " 'He feels very strongly that these two sites are dangerous.',\n",
       " 'He thinks that both sites are a health hazard risk due to obnoxious odours and in particular, the large fire that occurred last week which produced a lot of polluted smoke for a distance of six miles.',\n",
       " 'Some people reckoned that the smell of burning tyres could be smelt here in Gilgarran.',\n",
       " 'There have been numerous fires on these sites over the last few years.',\n",
       " 'These fires give rise to compaliant by people like us, but more so from the nearer village of Distington (1¾ miles west of here).',\n",
       " 'The councillor suggests that there could be more incidents of cancer cases in this area in coming years along with respiratory troubles as well as some cases of bronchitis related problems.',\n",
       " 'He himself has recently suddenly started sinusitis, which he hasn’t had before.',\n",
       " 'All in all, he wasn’t happy about the situation on both sites.',\n",
       " 'We don’t know what is being tipped there, all we can do as a community is accept what we are being told by the site owners.',\n",
       " 'As previously stated, animal carcasses were being tipped and buried for about three days before we were told officially that this was so.',\n",
       " 'Incidentally, the site where animals are buried is owned by Cumbria County Council.',\n",
       " 'This seems to be totally against the advice of County Council officials who look after the environment and the health of the population.',\n",
       " 'As I’ve written before, there are going to be bigger concerns if the opencast coal site to the south of the village becomes a landfill site for refuse from parts of the county fifty miles away.',\n",
       " 'At the moment there are no suggestions that anything from the FMD outbreak will be dumped there.',\n",
       " 'Having said that, however, we as villagers didn’t know of carcasses being buried or pyre ash being tipped until after it had happened.',\n",
       " 'We await the outcome of this coal site with some trepidation, after all, no coal has come from this site for some months.',\n",
       " 'It has all the indication of becoming a land fill site.',\n",
       " 'Week 6\\nMonday to Wednesday, if work is still ongoing at the burial site it is not visible from our side of the site.',\n",
       " 'I still don’t know what is going on there.',\n",
       " 'It may all be innocent and an improvement to the environment, after all, this is what the site owners have to do.',\n",
       " 'Thursday\\nA delegation of MEPs visit the north of the county.',\n",
       " 'They have come to assess the situation for themselves and to report back to the European Parliament.',\n",
       " 'No doubt they will also report back to their own constituents in their own countries.',\n",
       " 'The delegation visit the auction mart at Longtown where the disease was first noticed in this country and also visited the big burial site at Great Orton where it was estimated that half a million carcasses were buried.',\n",
       " 'Good coverage by the local press, radio and T.V.',\n",
       " 'gave anyone interested the views of the delegation.',\n",
       " 'Thursday – Saturday\\nThe MEP delegation agreed that the FMD situation had been disastrous (we all know that!).',\n",
       " 'Comments from some tourist and agriculture observers ranged from a ‘waste of time’ to ‘at least some politicians have bothered to visit us, our own couldn’t do that’ Personally, I think that some good came out of this, particularly when it was reported that the Dutch had used vaccination techniques when they had a small outbreak.',\n",
       " 'Many people think that the British Government should have had a public inquiry into the outbreak.',\n",
       " 'What have they to hide?',\n",
       " 'Cumbria is holding its own inquiry – quite rightly so, other organisations such as Lancaster University are holding research into the outbreak.',\n",
       " 'Why not the Government?',\n",
       " 'Eventually we will know why, perhaps not in my lifetime though.',\n",
       " 'The minister and MAFF have a lot to answer for.',\n",
       " 'Week 7\\nThought it would be of interest to include copies of the newsletter that the local authorities issued to every household in the area regarding the disposal of carcasses and effluent.',\n",
       " 'It will be of note that there was a fire last year on the Alco site, also involving tyres.',\n",
       " 'Very similar to last years only not as big.',\n",
       " 'A report on local T.V.',\n",
       " 'today stated that the recent visit of MEPs to the area considered that vaccination should have been used at the outset and be should seriously considered should a future outbreak occur.',\n",
       " 'Heard of reports of an outbreak of T.B.',\n",
       " 'in cattle in other parts of the country.',\n",
       " 'This was reported to be more serious than FMD should a major outbreak occur.',\n",
       " 'This would lead to the question of disposal should the need arise.',\n",
       " 'As I’ve already reported in previous entries, the use of the opencast coal site to the south-east of here is causing concern in some quarters.',\n",
       " 'Although the site didn’t feature in the FMD crisis, there is a feeling that it is being earmarked for use in the future should the need arise or even the rumour of an incinerator is planned for there.',\n",
       " 'The general feeling here and in the surrounding area is that we have had enough dumping of carcasses, effluent, toxic chemicals etc.',\n",
       " 'It could be that the authorities have seen that the sites concerned have handled those substances before, that an extension of disposal sites in this area would be effective.',\n",
       " 'Week 8\\nNothing of any significance to report this week.',\n",
       " 'Week 9\\nNow that Cumbria’s FMD inquiry has started, a lot of people I have met this week recall the happenings of a year ago.',\n",
       " 'Even more interesting is the coverage in the local press and T.V.',\n",
       " 'Plenty of publicity by the media shows how little the Government an MAFF in particular let the farming and tourism industries of the county down.',\n",
       " 'There has been plenty of distressing stories by farmers not only of infected animals being slaughtered but also the slaughtering of healthy animals in the 3 km circle of an outbreak.',\n",
       " 'One particularly distressing point of evidence was when a farmer described to the panel the birth of a calf five days after it’s mother had been shot!',\n",
       " 'We at the time of the outbreak were hearing these stories on a daily basis and still MAFF and Mr. Brown kept telling us that the outbreak was ‘under control’.',\n",
       " 'All I can say at this point is may heaven help us when it all happens again.',\n",
       " 'Week 10\\nWork is still going on at the burial site.',\n",
       " 'It looks like new soil is being dumped on top of the actual site and ‘dozed’ to level it of and to smooth it out on the side.',\n",
       " 'All we can do is accept that the management of the site are making it better for all concerned and that they are as concerned as we are.',\n",
       " 'The much publicised Cumbrian FMD inquiry team visited the land fill site.',\n",
       " 'They met local councillors who expressed their concern over this site and the Alco site.',\n",
       " 'No other report was forthcoming from the team.',\n",
       " 'The inquiry team finish their evidence gathering this week.',\n",
       " 'One very important statement was made that the Minister of the Environment should make a statement over this outbreak and should even make a visit to these sites county-wide.',\n",
       " 'There has been total silence from Mrs Beckett’s department over this request.',\n",
       " 'The same silence is observed from any government source for that matter.',\n",
       " 'Everyone asks the same questions, what have they got to hide?',\n",
       " 'Why aren’t they interested?',\n",
       " 'What plans are being made?',\n",
       " 'And what lessons have been learned from last years outbreak?',\n",
       " 'A lot of farms are restocking and in this neighbourhood, farm work is going on as before, or so it looks.',\n",
       " 'As time goes on though, there seems to be a smouldering anger that no-one in authority is as concerned as well are.',\n",
       " 'Week 11\\nWork is still on going at the burial site.',\n",
       " 'No comments heard from any of the villagers or neighbours this week.',\n",
       " 'Diary 12\\nMonday, from my own observation work is still ongoing at the burial site.',\n",
       " 'More heavy plant has been moved on to the top of the giant amount and it looks as though more topsoil is being laid over the Mount.',\n",
       " 'Perhaps to improve the site, but water may still permeate into and through the site.',\n",
       " 'We can only believe the operators that this is a right thing to do.',\n",
       " 'Friday, talked to 2 it villagers about the after-effects of FMD.',\n",
       " \"One said oh it's all over now and forgotten about it doesn't bother me one bit.\",\n",
       " 'The other said it all in the past we just have to forget about it.',\n",
       " \"It seems that life is returning to normal in all aspects of village life, people don't think about last year unless the diarist mentions that.\",\n",
       " 'Sunday, a bad day or weather wise.',\n",
       " 'This prolonged rain may halt work on the burial site.',\n",
       " 'Most people are reluctant to talk about F M D now, even if it was one of the worst economic and social disasters to hit this country and this County in particular.',\n",
       " \"Now that it is over, people's memories begin to fade.\",\n",
       " 'However, some of us are not happy at having these two disposal sites within a 1000 metres of this village.',\n",
       " 'FMD may be over but these burial sites are here for a long time yet.',\n",
       " 'Diary 13\\nObserved in work on burial site.',\n",
       " 'More heavy machinery and plant moved in and large quantities of soil are being laid down and smoothed out.',\n",
       " 'Diary 14\\nTalked to some religious today about the after-effects of FMD.',\n",
       " 'Without exception, they are not interested!',\n",
       " \"It's all over with an idle one to be reminded about it are the general comments.\",\n",
       " 'Nobody seems bothered that there are hundreds of animals buried a 1000 yards from his village or the fact that there is leachate and pyre ash buried in another site.',\n",
       " 'Looking at the burial site and the work that is going on there, it does look as though the management there are doing everything to make the site safe.',\n",
       " 'Diary 15\\nI met a smallholder today to whom I have talked to in the past about the effects and after-effects of FMD.',\n",
       " 'He still not happy about the burial site despite the landscaping and smoothing off of the large quantities of topsoil, only time will tell he says.',\n",
       " \"He does not have any stock near to the site but he has sheep on the farmer's land.\",\n",
       " 'Since FMD finished though his stock movements are still restricted by new legislation that has come in since the area was declared free.',\n",
       " 'For instance, or if he takes a sheep to auction, he asked to have nine pieces of paper for this transaction.',\n",
       " 'If the price is not right and he has to take the she back to his land he was put them back in the same field that they came from and it cannot move them to three weeks.',\n",
       " 'He then has to obtain a licence to do this.',\n",
       " 'He does think that the authorities are not going to be as strict shortly.',\n",
       " 'This is just one of the precautions that have come in to try and combat any recurrence of FMD.',\n",
       " 'Diary 16\\nI met the smallholder who rents land a from the farmer in the village.',\n",
       " 'His income from the sheep that he a breeds has been nil, like many more people in similar circumstances.',\n",
       " 'Fortunately for him had he has an income from another source.',\n",
       " 'The subject of compensation came up during our conversation.',\n",
       " 'I personally do not have any comment to make about this item as it maybe just a rumour, apparently he got it bee in his bonnet about compensation paid out to people who were not in the agricultural business.',\n",
       " 'What seemed to upset him was that he had heard that some of fish and chip shop owner in the Lake District had been paid £170 per month compensation for the loss of trade.',\n",
       " \"He didn't mind too much that hoteliers and guest house owners had claimed compensation, but wondered where else would this kind of money go when he himself had been paid nothing.\",\n",
       " \"This is the first time I've heard this one!\",\n",
       " 'Diary 17\\nAttended the Cumberland Show at every to be park Carlisle.',\n",
       " 'We, as a family used to attend this annual show regularly, both as spectators and competitors.',\n",
       " 'We have never seen the show like the one put on this year, when will things really get back to normal?',\n",
       " 'Many of us think that agriculture is back to pre-FMD.',\n",
       " 'Cattle and sheep on grazing in the fields, lambing has reached new heights in produce on some farms.',\n",
       " 'Calves are being born, silage and haymaking is progressing when the weather permits.',\n",
       " 'But there are still restrictions on animal movements.',\n",
       " \"Hence, no sheep cattle or pigs at this year's show only horses, poultry, dogs and rabbits.\",\n",
       " 'Not many pieces of agricultural machinery onshore either.',\n",
       " 'Plenty of Chartered Accountants tents, craft tents, Horse feeds and tack, displays in the main arena and bands.',\n",
       " 'Not an agricultural show as we knew it.',\n",
       " 'It seems to be the same at other shows, Ennerdale show is one of our local shows.',\n",
       " \"This year there isn't going to be any horses or sheep.\",\n",
       " \"Generally there are no cattle shown at the show, but without sheep (hill farmers dominate the show) the there isn't going to be much on show at all.\",\n",
       " 'It was always a good show for equestrian events at many levels.',\n",
       " 'This show was always a must for our family.',\n",
       " \"I don't think that we will be going this year.\",\n",
       " 'Diary 18\\nFrom the golf course and golf driving range I can look out on to the western side of the burial site.',\n",
       " 'I have written in previous weeks about the work there has been going on at this site.',\n",
       " 'Viewing the site are from our village side, would hardly know what that there ever was a burial site.',\n",
       " 'Hundreds of tons of topsoil had been laid and smoothed out to make more-or-less like a landscaped feature.',\n",
       " 'It looks really good.',\n",
       " 'From the western side though, things are little different.',\n",
       " 'Work is still going on there, large amounts of soil have been tipped and levelled off.',\n",
       " 'There are still Portakabins there and heavy plant can still be seen moving about.',\n",
       " 'No doubt the western side well look as good as the eastern side before long.',\n",
       " 'Diary 19\\nIt is announced that the Prime Minister and his wife and son of his family at a visit to Cumbria.',\n",
       " 'The PM arrives in West Cumbria, all kinds of reports are written in the local and national press about what he is going to do or not do or what he should be doing.',\n",
       " 'After all, he is on holiday.',\n",
       " \"The PM did meet some farmers' leaders, the press as usual stirred things up or as to where he should be meeting.\",\n",
       " 'Tourism officials say that the trip was fantastic for tourism in the county.',\n",
       " \"Or person they I can't see what difference it made.\",\n",
       " 'If people want to come Cumbria, they will come irrespective of whether the PM comes or not.',\n",
       " 'Diary 20\\nAfter a lot of protests it looks as though it the 20 day restriction on cattle Movement will be lifted.',\n",
       " 'Perhaps this will now mean that they could be cattle and sheep entries at local agricultural shows.',\n",
       " 'Some shows are going ahead with very limited entries of livestock and some with no animal entries at all.',\n",
       " 'These shows have always been very popular with my family for over 20 years.',\n",
       " 'Also, living with in a farming community makes us feel part of the annual agricultural scene.',\n",
       " 'Diary 21\\nI’ve written before regarding agricultural shows and the pride in which local people take in these shows.',\n",
       " 'Although a lot of shows have gone ahead this season, they have had a reduced animal showing or in some cases no animals at all.',\n",
       " 'Today I’ve heard that one show has been cancelled altogether.',\n",
       " 'This particular show is one of the most popular in the area.',\n",
       " 'Maybe because of lack of entries or the organisers just wanted to cancel because of the 3 week restriction on animal movement.',\n",
       " 'I don’t know.',\n",
       " 'Perhaps it would be better to cancel them than have a depleted show.',\n",
       " 'Diary 22\\nSpent a few hours in the fells today.',\n",
       " 'It was good to be able to wander the familiar paths and let our dog run free.',\n",
       " 'It was a good boost to our moral and perhaps the dog’s too!',\n",
       " 'We all missed being able to do this last year.',\n",
       " 'Diary 23\\nLast Bank Holiday before Xmas and the last before the schools go back.',\n",
       " 'At the golf course where I help out part-time during the summer we had lots of customers.',\n",
       " 'A lot of them commented on how enjoyable it was to be on holiday in this area this year, compared to the restrictions that were in place last year.',\n",
       " 'Maybe the holiday establishments are getting back to normal.',\n",
       " 'There are no restrictions put on them like there is in place now with farmers and agriculture.',\n",
       " 'Diary 26\\nSorting through the mail left whilst away on holiday and I came across a notice sent by the village committee notifying a Harvest Thanksgiving festival to be held next month in the village hall.',\n",
       " 'As we have no church in the village, it is being held in some farm buildings in the centre of the village.',\n",
       " 'This will be a splendid event.',\n",
       " 'The farm did not have FMD but couldn’t take animals from one field to another and couldn’t market them.',\n",
       " 'When we consider the gloom that settled on this farm and community, it is very welcome to have this unique event here in the heart of the village and the farmer and his wife will be at the centre of events.',\n",
       " 'A lovely gesture and I hope it will be well supported.',\n",
       " 'There will be a distribution of harvest gifts afterwards, what a change from a year ago!',\n",
       " 'Diary 27\\nWith the aid of binoculars I have been able to have a closer look at the burial site from a westerly direction.',\n",
       " 'There are vents in the shape of small towers to extract gas from the site.',\n",
       " 'There are pipes connecting these vents.',\n",
       " 'A lot of work is still going on there.',\n",
       " 'However, all this takes place in the Western side which is the opposite side to where my village is situated.',\n",
       " 'From our side there is nothing to suggest the amount of work going on.',\n",
       " 'Because of this, FMD is pushed further into the backs of villager’s minds.',\n",
       " 'It is something in the past.',\n",
       " 'It has happened, so what?',\n",
       " 'People like myself who talk to farmers and agriculturalists do not easily forget these events.',\n",
       " 'Personally I am still concerned about the burial site.',\n",
       " 'When inquiries are made about it, all we can do is accept what we are told.',\n",
       " 'It does not look as though every precaution is being taken to alleviate an odours or contamination.',\n",
       " 'Diary 28\\nI had to see the village farmer on another matter and was asked inside for coffee and a chat.',\n",
       " 'He was able to tell me of the full implications of the ’20 day rule’.',\n",
       " 'He accepts that this is a precaution to prevent another outbreak of FMD, but there is a lot of work involved.',\n",
       " 'He told me of an isolation area that he has created and also the fencing arrangements where his land adjoins the neighbours land.',\n",
       " 'I would say that 95% of the public don’t know about this even if they have heard of the 20 day rule.',\n",
       " 'For him (he owns the largest farm in the area) it is bad enough having to do all the physical work as regards fencing, etc.',\n",
       " 'But for anyone such as a small holder, it must be a nightmare if he has to bring animals back from market that haven’t been sold.',\n",
       " 'Friday, my wife and I played a round of golf at Aspatria.',\n",
       " 'This course was badly restricted when FMD hit this area.',\n",
       " 'We were reminded that there are restrictions on adjoining land.',\n",
       " 'There were notices asking people who hit balls onto farm land not to cross the fence to retrieve them because of FMD precautions.',\n",
       " 'This was news to us.',\n",
       " 'It does make sense though.',\n",
       " 'The farmer wouldn’t know where players had been walking prior to playing golf.',\n",
       " 'Diary 29\\nAttended the Harvest Festival held in  the village farm, a large cattle shed had been cleaned and decorated for this event, chairs had been brought in, fruit and vegetables were on display for auctioning at the end.',\n",
       " 'The place was packed!',\n",
       " 'A lot of money was raised and it was a very happy event, well supported and a big boost for the farm and the village.',\n",
       " 'I don’t think that the general public care much about FMD now that is has been a year since the last case was confirmed in Cumbria.',\n",
       " 'The public may be reminded if they read the local newspapers intently, for instance, there was a letter to the editor published recently which referred to the results of the Cumbria Inquiry into FMD.',\n",
       " 'It may have been a farmer who wrote it, I don’t know, but the writer certainly went to town in the scathing comments on the handling of FMD.',\n",
       " 'Even caustic remarks regarding the efforts since FMD of DEFRA and Mrs Beckett.',\n",
       " 'I certainly wouldn’t like to cross the writer, I also think the farming community must be holding it’s breath in case the present restrictions such as they are, prove to be worthless.',\n",
       " 'Then we will all suffer, again!',\n",
       " 'Week 30\\nWhat a difference a year makes!',\n",
       " 'Despite some restrictions on public access to agricultural fields in some areas of the county, it doesn’t apply here.',\n",
       " 'Although most locals confine themselves to footpaths and bridleways, other people seem to think that all fields are recreation areas.',\n",
       " 'They walk and run across some of the fields in close proximity to the village.',\n",
       " 'Regardless of the presence of stock they exercise dogs and treat it as a some kind of park.',\n",
       " 'One farmer is well know for being aggressive, he used last year’s FMD outbreak to run people off his land.',\n",
       " 'I met a local councillor who expressed concerns regarding the proposed building of an incinerator to the south of the village on the current open cast mining site.',\n",
       " 'The two waste disposal sites to the west and north-west of the village have become big issues in the last 18 months due to the burial of animals and the disposal of pyre ash and leachates.',\n",
       " 'It seems as though we are going to get over this ghastly FMD outbreak only to have this scenario thrust upon us.',\n",
       " 'Week 31\\nMet a small-holder who keeps sheep near to this village.',\n",
       " 'He was very scathing over the report that the Government and DEFRA don’t want to talk up an offer from the Local Authorities here to implement findings and recommendations from their local inquiry over FMD.',\n",
       " 'Why?',\n",
       " 'What has this Government, who didn’t perform very well during the outbreak, got to hide and why shirk away from the findings instead of facing up to the failings that we all know about.',\n",
       " 'It also seems that they don’t want to make any safeguards and recommendations to avoid a further outbreak.',\n",
       " 'As a non-agriculturalist, it doesn’t surprise me in the least.',\n",
       " 'After all, Government has failed other industries in the country for as long as I can remember.',\n",
       " 'Week 32\\nI am convinced that authorities in the area must think that the way animals were buried here and pyre ash and leachate were disposed of at another site nearby was all done as very successfully and that the two sites handled everything professionally.',\n",
       " 'Therefore the sites would be more than capable of handling ash from an incinerator.',\n",
       " 'To me, this is the legacy of FMD.',\n",
       " 'I am most annoyed over this, together with a lot more of the villagers.',\n",
       " 'This village no longer has a representative on the parish council, both have resigned for whatever reason and no-one will step forward to take it one.',\n",
       " 'I have said that I would take a set on the parish council to represent the village and fight for our rights and future quality of life.',\n",
       " 'Due to this, I have uncovered a pile of claims and counter-claims.',\n",
       " 'It seems that both parish and district counsellors know what is going on (regarding the incinerator) and that developers have made ‘concessions’ to some councillors.',\n",
       " 'Also there are claims that the developers have offered money to local landowners and farmers so that roads can be put in.',\n",
       " 'All these accusations have been strongly denied.',\n",
       " 'At the same time it is rumoured that some farmers have been offered local fields nearby.',\n",
       " 'Because of what I have discovered in my own investigations, it would seem that a lot of friendships gained over 20 years could come to an end, I am fearful of what I have uncovered.',\n",
       " 'There are also claims that ‘councillors are only in it for what there can get out and are not to be trusted.’ I don’t want that said of me.',\n",
       " 'Also by the time all this is sorted out, I will be 70-75,  I certainly don’t want to be fighting peoples battles at that age.',\n",
       " 'However, I will support any effort to stop the proposed development.',\n",
       " 'Week 33\\nOnce again the large farm in the centre of the village was the venue for the annual Guy Fawkes bonfire and fireworks.',\n",
       " 'Organisers had been round the village asking for donations to provide fireworks.',\n",
       " 'A tractor and trailer toured the areas picking up things for the bonfire.',\n",
       " 'Drinks and food were served in a barn after the fireworks.',\n",
       " 'This is another occasion when villagers and the farming community come together.',\n",
       " 'It is perhaps the only time that the general public of the village think about FMD and last years events, if only briefly.',\n",
       " 'The farmer remarked that is the third time this year that there has been a public function on his farm.',\n",
       " 'The first was the jubilee party in June, then on October 6th the Harvest Festival service.',\n",
       " 'These events keep farming in the public eye.',\n",
       " 'Week 34\\nI haven’t written before about the proposed building of an incinerator nearby to burn the counties waste.',\n",
       " 'If , as we all suspect, the incinerator is built, then the odours plus the disposal of ash (to the FMD waste site) is a legacy of FMD, particularly regarding the nearby burial and disposal site.',\n",
       " 'Week 35\\nThis is week 35 of this project and for most of the 35 weeks I have written that I am not confident of the future.',\n",
       " 'There are numerous reasons for this.',\n",
       " 'Mainly the situation in the Middle East.',\n",
       " 'Today I travelled to Keswick to do some Xmas shopping.',\n",
       " 'I was given a lift there by a neighbour who is in his 30s.',\n",
       " 'He was very upset about the terrorist situation, not only was he concerned about the terror threat to the London Underground, but the threat closer to home as regards a plane crashing into the nearby Sellafield complex.',\n",
       " 'We don’t know the effect that this constant bad news has on people.',\n",
       " 'People who have already got serious worries, e.g., families, housing, finance, etc must feel really depressed about it all.',\n",
       " 'Week 36\\nNear to the next village is a long established farm of many acres.',\n",
       " 'Recently the farm’s stock of animals and machinery was sold off.',\n",
       " 'The owner, who had farmed for sixty years was leaving to live with one of his brothers.',\n",
       " 'He said that he wouldn’t know how he would feel when he left the farm for the last time this weekend.',\n",
       " 'The farmhouse hasn’t been sold yet and now stands empty.',\n",
       " 'It’s a strange place now, where everything was hustle and bustle (they even had a B & B business there) is now derelict and bare.',\n",
       " 'It’s a sad reflection on the agricultural business in the wake of FMD.',\n",
       " 'This farm isn’t the only one in the area that has sold up.',\n",
       " 'Some farm houses remain as dwellings, but this particular one which we saw nearly every day is just an other sad reminder of the way farming has declined in this rural area.',\n",
       " 'Week 39\\nTuesday.',\n",
       " 'Boarded the train  at Penrith to journey to Crewe to see our daughter.',\n",
       " 'During the journey I got into conversation with a fellow passenger.',\n",
       " 'He noticed I had got on the train at Penrith and perhaps thought I was connected with the agricultural industry.',\n",
       " 'The conversation drifted into the previous years FMD outbreak.',\n",
       " 'It is rather strange, that I live in a very rural area, and , FMD is rarely mentioned now.',\n",
       " 'However, this fellow passenger (although not from an agricultural background) gave his views on the handling of the situation.',\n",
       " 'It was no different from the views expressed by locals at the time of the crisis.',\n",
       " 'It just goes to show that FMD is very much in peoples minds even if they were not connected to agriculture in any way.',\n",
       " 'Week 40\\nFriday.',\n",
       " 'Now that the MEP have published their critical report on the FMD crisis, it is interesting to read an article published in our local weekly paper, from a reader … (Article entitled ‘Foot and Mouth Report’ included).',\n",
       " 'I don’t have the knowledge or the data to support this readers comments.',\n",
       " 'However, I have heard plenty of stories from mainly unreliable sources, to confirm what he says.',\n",
       " 'It makes interesting reading I think.',\n",
       " 'Week 41\\nTuesday.',\n",
       " 'No wonder my confidence in the future has taken a big plunge over the last few months.',\n",
       " 'The situation in Iraq doesn’t get any better.',\n",
       " 'Mr Tony Blair’s message to the Armed Forces of the UK bear this out.',\n",
       " 'Being an ex-serviceman, I know what the situation holds for our troops.',\n",
       " 'But, are we right to follow the USA in a war against Iraq?',\n",
       " 'No doubt Saddam Hussein does pose a threat but so does India and Pakistan to each other.',\n",
       " 'Each of these two relatively poor countries has threatened each other as regards their nuclear arsenals.',\n",
       " 'Now, the loose cannon in the form of North Korea is positioning itself as regards its position in the nuclear arms league.',\n",
       " 'Personally, I think that North Korea poses a more dangerous threat than Iraq.',\n",
       " 'It is not a very happy New Year for a lot of people.',\n",
       " 'Perhaps it will all be settled diplomatically.',\n",
       " 'I wonder.',\n",
       " 'Week 42\\nNothing of any importance to write about due to refurbishment at home.',\n",
       " 'Week 43\\nMonday.',\n",
       " 'One of the items on the agenda for this months meeting of Distington Parish Council is a report on the wood-felling and the implications of this.',\n",
       " 'As I have written in the diary before, there are strong rumours of the proposed plan to fell woods, build a new road through the felled site and bring coal from the nearby opencast site to link up with an existing road, then, to transport the coal to a storage area on Workington Dock.',\n",
       " 'Then, when the coal is worked out, to build an incinerator on the coal site.',\n",
       " 'Ash from this development would then be transported on the ‘new’ road to be disposed of on the waste disposal site that was used for FMD pyre ash and leachate.',\n",
       " 'Thursday.',\n",
       " 'Read a report of the aforesaid meeting.',\n",
       " 'The owners have declared that our worries are groundless.',\n",
       " 'In fact, they say that they plan to eventually open the woodland to the public (the owners of the woodland are the same operators of the opencast coal site).',\n",
       " 'Footpaths will be created if a grant can be obtained.',\n",
       " 'A wooden wheeled ancient water mill will be restored.',\n",
       " 'After the closed meeting the operations director of the site said that ‘There has been a misunderstanding.',\n",
       " 'What we are doing will benefit local people’.',\n",
       " 'He said that a management project for the wood is being followed involving felling dead trees and fresh planting.',\n",
       " 'He added: “The felling and replanting will be done this year after which it will take time to become established.',\n",
       " 'We’re talking of a ten year programme but it should have long-term benefits.',\n",
       " 'I think our PR at the start of this wasn’t very good and in the future we will let the council know of our plans”.',\n",
       " 'The council agreed to keep a watch on the work here in G.\\nThis statement differs greatly from what some of us have been told by our village-based County Councillor.',\n",
       " 'There has never been any suggestion that the felled woods would become a land fill site, but would be felled to provide the new road.',\n",
       " 'There was nothing mentioned at the meeting regarding the proposed incinerator being built.',\n",
       " 'The County Council … that this has ever been planned.',\n",
       " 'However, our representative is adamant that this is not so.',\n",
       " 'Week 44\\nTuesday.',\n",
       " 'For the first time, my property has finally overcome a situation that was affected by FMD.',\n",
       " 'In July 2000, the electricity supplier notified me to say that the trees in my garden had grown so tall that the topmost branches were in close contact with an eleven thousand volt overhead power line and that they should be felled or severely pruned.',\n",
       " 'After some further negotiations it was decided to prune to some height that I wasn’t happy with.',\n",
       " 'Although the treetops were not actually touching the wires, it was considered a risk in the forthcoming months.',\n",
       " 'However, as time passed I couldn’t wait for the foresters to arrive, so I pruned the trees myself.',\n",
       " 'In January 2001, the electric supplier suggested that the trees should be pruned further.',\n",
       " 'A date was agreed but the foresters didn’t arrive.',\n",
       " 'Time dragged on and the trees grew back to their original height.',\n",
       " 'Again, the electric supplier suggested they be pruned or felled.',\n",
       " 'A new date was agreed upon.',\n",
       " 'However, the foresters couldn’t do the job because the isolator switch was on farmland and they couldn’t get access to it because of FMD restrictions.',\n",
       " 'And so it dragged on!',\n",
       " 'Despite visits by foresters and electric supplier reps. the trees got bigger and I was forbidden to touch them.',\n",
       " 'Neighbours could hear crackling noises coming from the wires and it became very worrying.',\n",
       " 'People suggested that I should ‘do something about it’.',\n",
       " 'I took the matter up directly with the supplier and the foresters.',\n",
       " 'I was promised dates only for them to be cancelled.',\n",
       " 'In December 2002, a date of 21st January 2003 was given.',\n",
       " 'This time, they came and we agreed that two trees be felled and another pruned.',\n",
       " 'After 30 months it finally happened.',\n",
       " 'Thursday.',\n",
       " 'Met a small holder who has his land on the edge of this village, who told me that the 20 day rule of animal restriction of animal movement was being lifted and replaced by a 6 day restriction.',\n",
       " 'This was good news for him and any other farmer.',\n",
       " 'Later that day I met another farmer who didn’t know that the restriction was being lifted.',\n",
       " 'You would have thought that I had told him he’d won the lottery!',\n",
       " 'Good news all round for the people.',\n",
       " 'Friday.',\n",
       " 'Listening to the local radio today and was surprised to hear a report that the Citizens Advice Bureau in a small Lakeland town had been receiving clients who were still experiencing hardship due to FMD.',\n",
       " 'It is now 18 months since the last outbreak and the effects (according to the person being interviewed) were still being felt.',\n",
       " 'Not just by farmers and agriculturists, but by guest houses, hotels, tradesmen and in particular, some self employed.',\n",
       " 'Debt seems to be the biggest problem.',\n",
       " 'It seems as though some people had weathered the hardships of FMD initially only to find that their plans had come adrift somehow afterwards.',\n",
       " 'Quite disturbing to hear that the situation is still with us in this county to some degree.',\n",
       " 'Week 45\\nThese diaries were instituted to deal with the after effects of FMD.',\n",
       " 'Although there were no cases of FMD in this village, everyone knew about it, particularly as nearly everyone who went to work from here would pass the main farm in the village centre, or, some of the farms on the outskirts of the village.',\n",
       " 'Now that FMD is over, most people who live here don’t seem to think about it anymore.',\n",
       " 'The only people affected are the farmers, naturally.',\n",
       " 'This is a strange village in lots of ways.',\n",
       " 'Only the farmer and his immediate family are connected with agriculture.',\n",
       " 'The rest are professional people or people who work at nearby Sellafield, industries in Workington and Whitehaven, or are retired.',\n",
       " 'There is no church, no village pub, no village shop, no village community centre or meeting place.',\n",
       " 'Only tradesmen that call are the milkman and the solid fuel merchant.',\n",
       " 'We are left to ‘get on with life’ in our own way.',\n",
       " 'The parish of Distington to which we belong have all the facilities associated with a larger community, such as a church, pub and community centre.',\n",
       " 'All of which are two miles away.',\n",
       " 'Consequently, the Parish Council meets there once a month and discusses all the problems of the area including ours.',\n",
       " 'However, our representative on the council has resigned and no-one has come forward to represent us.',\n",
       " 'Anything that has been discussed at the Parish Council is reported in he local newspaper.',\n",
       " 'Village pubs are a good venue to discuss local issues and to exchange views and, mainly, to gossip.',\n",
       " 'Village ‘tittle tattle’ as I call it!',\n",
       " 'As we have no pub, the gossip is rife from one source or another with bits added on or left out as is the choice of the person concerned.',\n",
       " 'Quite a lot of people one meets are ‘experts’ in their own particular choice of subject whether it is politics, finance or Mrs Jones current boy friend.',\n",
       " 'It is a fault to take on board all that is gossiped about when one meets a fellow villager in the country lanes whilst out walking the dog.',\n",
       " 'Week 46\\nIllness to a family member.',\n",
       " 'Week 47\\nContinued illness.',\n",
       " 'Week 48\\nOver the past few weeks there has been a lot of tree felling in the nearby woods.',\n",
       " 'This has led to a lot of disturbance to the villagers because of the use of large vehicles needed to remove the felled timber and also the foresters vehicles churning up the grass verges and the ditches.',\n",
       " 'A lot of concern was raised about the necessity of all the tree felling.',\n",
       " 'These concerns were raised in the press and also in the parish council.',\n",
       " '(I have written about these in diaries in the last few weeks).',\n",
       " 'It was reported in mid-January that all the felled woods would be replanted this year, with footpaths created for the enjoyment of the local population.',\n",
       " 'Now, all timber operations have ceased.',\n",
       " 'Large areas of woodland have been left partly felled and a lot of felled timber is left lying about.',\n",
       " 'Foresters vehicles have gone and nothing is happening.',\n",
       " 'Despite assurances from the developers, it looks as though something drastic has happened.',\n",
       " 'Village ‘tittle tattle’ says that the foresters have not been paid for their work so far and that the developers have run out of money.',\n",
       " 'If this is so, what is going to happen now?',\n",
       " 'When felling started late last year, I contacted two environmental agencies regarding the threat to the red squirrels, badgers and buzzards that occupy these woods.',\n",
       " 'I was told that it was only a partial felling and they (the environmental agencies) were satisfied that any disturbances would be slight.',\n",
       " 'I think that they were told this by the developers, and accepted what they were told without a site visit.',\n",
       " 'The developers have been known to mislead groups in the past, including landowners, farmers, councils and individuals.',\n",
       " 'I, personally am not happy about this situation.',\n",
       " 'I have always took a keen interest in wildlife and feel that we have been let down by the lies of developers and the lack of serious interest from wildlife agencies, some of which are an offshoot of central Government.',\n",
       " 'I for one will keep a close look on the situation with or without other villagers and in particular, local councillors.',\n",
       " 'Week 49\\nBy chance I met three small holders all at the same time.',\n",
       " 'They were discussing farming by the roadside.',\n",
       " 'All of them were pleased that the 20-day ruling was coming to an end and that their lives were more or less coming back to normal.',\n",
       " 'They also expressed the opinion that the 20-day rule and the 6-day rule were only in force to protect their interests.',\n",
       " 'However, they were unanimous in their condemnation over the importing of foreign meat and meat products into this country.',\n",
       " 'They feel that foreign meat is not subjected to enough checks before entry into the United Kingdom.',\n",
       " 'Week 51\\nMet a farmer today who told me that he’d seen a report based on findings by the EU and DEFRA.',\n",
       " 'It stated all the things that everyone who is an agriculturalist and those who take an interest in the countryside had been saying about what was wrong with the handlers of the FMD outbreak.',\n",
       " 'It just proves that it doesn’t take an academic genius to know what should have been done at the time.',\n",
       " 'Everyone can be wiser after the event, but statements by the NFU and individuals at the onset were not heeded.',\n",
       " 'For example, the movement of animals should have been halted sooner and the Army should have been brought in much sooner.',\n",
       " 'Now, the question of vaccination rumbles on.',\n",
       " 'Should we or shouldn’t we vaccinate?',\n",
       " 'There is a fear of the outbreak again, particularly when the findings of the 1960 outbreak were not implemented.',\n",
       " 'Since the sadness of FMD, there has been quite a few instances of socialising at the farm, such as harvest festival, jubilee party and almost any excuse for a ‘shindig’, good to see farmers enjoying themselves.',\n",
       " 'Week 52\\nMet out local farmer who told me that there is to be new legislation to dispose of fallen stock.',\n",
       " 'No longer can a farmer bury fallen stock on his land, but must now provide an incinerator to dispose of dead animals.',\n",
       " 'This must be a costly business, could dead animals not be taken to a central point and burned?',\n",
       " 'Week 54\\nOne thing about FMD was the effect that it had on the poaching fraternity.',\n",
       " 'Living in a rural area, we expect this to happen.',\n",
       " 'Nobody seems to mind that a few rabbits and pheasants go missing.',\n",
       " 'What is really alarming is the use of dogs and high-powered rifles to poach deer.',\n",
       " 'FMD put a stop to all this.',\n",
       " 'Now a neighbour has told me of poachers near to the village using rifles and shooting deer.',\n",
       " 'The only people benefiting from this are the poachers and hoteliers who receive these dead beasts and no questions asked.',\n",
       " 'Also the danger of villagers being hit by stray rifle shots causes alarm to others.',\n",
       " 'Week 55\\nI think that there is a lot of jumping on the band wagon now that FMD has cleared up.',\n",
       " 'For instance, I listened to an interview on the local radio station given by a hotelier.',\n",
       " 'Things weren’t going well in his establishment.',\n",
       " 'Having got over FMD and its implications, visitors were slowly returning to the area, but not in sufficient numbers to cause great joy.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth['sent_tokenised'] = oc_foot_mouth.apply(lambda row: nltk.sent_tokenize(row['everything_else']), axis=1)\n",
    "oc_foot_mouth.loc[5, 'sent_tokenised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sent_tokenised</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[\\n\\nInformation about diarist\\nDate of birth:...</td>\n",
       "      <td>[0.0, 0.125, 0.0, 0.0, 0.39999999999999997, 0....</td>\n",
       "      <td>[0.0, 0.375, 0.125, 0.17777777777777778, 0.466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 196...</td>\n",
       "      <td>[-0.19722222222222222, -0.3186111111111111, 0....</td>\n",
       "      <td>[0.3972222222222222, 0.5777777777777777, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[\\n\\nInformation about diarist\\nDate of birth:...</td>\n",
       "      <td>[0.11742424242424243, 0.0, 0.31145833333333334...</td>\n",
       "      <td>[0.28030303030303033, 0.0, 0.4166666666666667,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 196...</td>\n",
       "      <td>[0.3666666666666667, 0.5, 0.0, -0.051851851851...</td>\n",
       "      <td>[0.16666666666666666, 0.15, 0.0, 0.51851851851...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 198...</td>\n",
       "      <td>[-0.0625, 0.5, -0.024999999999999994, 0.0, 0.0...</td>\n",
       "      <td>[0.3875, 0.5, 0.225, 0.0, 0.0, 0.25, 0.0, 0.25...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "\n",
       "                                      sent_tokenised  \\\n",
       "0  [\\n\\nInformation about diarist\\nDate of birth:...   \n",
       "1  [Information about diarist\\nDate of birth: 196...   \n",
       "2  [\\n\\nInformation about diarist\\nDate of birth:...   \n",
       "3  [Information about diarist\\nDate of birth: 196...   \n",
       "4  [Information about diarist\\nDate of birth: 198...   \n",
       "\n",
       "                                            polarity  \\\n",
       "0  [0.0, 0.125, 0.0, 0.0, 0.39999999999999997, 0....   \n",
       "1  [-0.19722222222222222, -0.3186111111111111, 0....   \n",
       "2  [0.11742424242424243, 0.0, 0.31145833333333334...   \n",
       "3  [0.3666666666666667, 0.5, 0.0, -0.051851851851...   \n",
       "4  [-0.0625, 0.5, -0.024999999999999994, 0.0, 0.0...   \n",
       "\n",
       "                                        subjectivity  \n",
       "0  [0.0, 0.375, 0.125, 0.17777777777777778, 0.466...  \n",
       "1  [0.3972222222222222, 0.5777777777777777, 0.0, ...  \n",
       "2  [0.28030303030303033, 0.0, 0.4166666666666667,...  \n",
       "3  [0.16666666666666666, 0.15, 0.0, 0.51851851851...  \n",
       "4  [0.3875, 0.5, 0.225, 0.0, 0.0, 0.25, 0.0, 0.25...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth['polarity'] = oc_foot_mouth['sent_tokenised'].apply(lambda x: [TextBlob(y).sentiment.polarity for y in x])\n",
    "oc_foot_mouth['subjectivity'] = oc_foot_mouth['sent_tokenised'].apply(lambda x: [TextBlob(y).sentiment.subjectivity for y in x])\n",
    "\n",
    "oc_foot_mouth[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth['pol_filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['polarity']]\n",
    "oc_foot_mouth['subj_filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Filename</th>\n",
       "      <th>everything_else</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sent_tokenised</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>pol_filtered</th>\n",
       "      <th>subj_filtered</th>\n",
       "      <th>pol_mean</th>\n",
       "      <th>subj_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5407diary02.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1975, gend...</td>\n",
       "      <td>[\\n\\nInformation about diarist\\nDate of birth:...</td>\n",
       "      <td>[0.0, 0.125, 0.0, 0.0, 0.39999999999999997, 0....</td>\n",
       "      <td>[0.0, 0.375, 0.125, 0.17777777777777778, 0.466...</td>\n",
       "      <td>[0.125, 0.39999999999999997, 0.183333333333333...</td>\n",
       "      <td>[0.375, 0.125, 0.17777777777777778, 0.46666666...</td>\n",
       "      <td>0.116305</td>\n",
       "      <td>0.444695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5407diary03.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1966...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1966, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 196...</td>\n",
       "      <td>[-0.19722222222222222, -0.3186111111111111, 0....</td>\n",
       "      <td>[0.3972222222222222, 0.5777777777777777, 0.0, ...</td>\n",
       "      <td>[-0.19722222222222222, -0.3186111111111111, 0....</td>\n",
       "      <td>[0.3972222222222222, 0.5777777777777777, 0.687...</td>\n",
       "      <td>0.082216</td>\n",
       "      <td>0.511248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5407diary07.rtf</td>\n",
       "      <td>\\n\\nInformation about diarist\\nDate of birth: ...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[\\n\\nInformation about diarist\\nDate of birth:...</td>\n",
       "      <td>[0.11742424242424243, 0.0, 0.31145833333333334...</td>\n",
       "      <td>[0.28030303030303033, 0.0, 0.4166666666666667,...</td>\n",
       "      <td>[0.11742424242424243, 0.31145833333333334, 0.2...</td>\n",
       "      <td>[0.28030303030303033, 0.4166666666666667, 0.31...</td>\n",
       "      <td>0.197901</td>\n",
       "      <td>0.500509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5407diary08.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1963...</td>\n",
       "      <td>Group 6</td>\n",
       "      <td>[information, diarist, date, birth, 1963, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 196...</td>\n",
       "      <td>[0.3666666666666667, 0.5, 0.0, -0.051851851851...</td>\n",
       "      <td>[0.16666666666666666, 0.15, 0.0, 0.51851851851...</td>\n",
       "      <td>[0.3666666666666667, 0.5, -0.05185185185185181...</td>\n",
       "      <td>[0.16666666666666666, 0.15, 0.5185185185185186...</td>\n",
       "      <td>0.113457</td>\n",
       "      <td>0.481575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5407diary09.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1981...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1981, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 198...</td>\n",
       "      <td>[-0.0625, 0.5, -0.024999999999999994, 0.0, 0.0...</td>\n",
       "      <td>[0.3875, 0.5, 0.225, 0.0, 0.0, 0.25, 0.0, 0.25...</td>\n",
       "      <td>[-0.0625, 0.5, -0.024999999999999994, 0.25, 0....</td>\n",
       "      <td>[0.3875, 0.5, 0.225, 0.25, 0.25, 1.0, 0.333333...</td>\n",
       "      <td>0.148358</td>\n",
       "      <td>0.503865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5407diary10.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1937...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1937, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 193...</td>\n",
       "      <td>[0.0, -0.13333333333333333, 0.0, -0.1, 0.0, -0...</td>\n",
       "      <td>[0.0, 0.4666666666666666, 0.0, 0.0333333333333...</td>\n",
       "      <td>[-0.13333333333333333, -0.1, -0.14, 0.4, -0.06...</td>\n",
       "      <td>[0.4666666666666666, 0.03333333333333333, 1.0,...</td>\n",
       "      <td>0.085418</td>\n",
       "      <td>0.454015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5407diary13.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1947...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1947, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 194...</td>\n",
       "      <td>[0.0, -0.5, 0.0, 0.0, -0.1875, 0.375, 0.0, 0.5...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.5333333333333333, 0.3125, 0....</td>\n",
       "      <td>[-0.5, -0.1875, 0.375, 0.5, -0.324999999999999...</td>\n",
       "      <td>[1.0, 0.5333333333333333, 0.3125, 0.75, 0.5, 0...</td>\n",
       "      <td>0.165448</td>\n",
       "      <td>0.487512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>5407diary14.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1964, gend...</td>\n",
       "      <td>[\\nInformation about diarist\\nDate of birth: 1...</td>\n",
       "      <td>[0.0, 0.2, 0.10000000000000002, 0.0, 0.0499999...</td>\n",
       "      <td>[0.0, 0.85, 0.3833333333333333, 0.0, 0.2499999...</td>\n",
       "      <td>[0.2, 0.10000000000000002, 0.04999999999999999...</td>\n",
       "      <td>[0.85, 0.3833333333333333, 0.24999999999999997...</td>\n",
       "      <td>0.123176</td>\n",
       "      <td>0.492388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5407diary15.rtf</td>\n",
       "      <td>Information about diarist\\nDate of birth: 1949...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1949, gend...</td>\n",
       "      <td>[Information about diarist\\nDate of birth: 194...</td>\n",
       "      <td>[0.0, 0.0, 0.2875, 0.0, 0.3509259259259259, 0....</td>\n",
       "      <td>[0.3333333333333333, 0.0, 0.8, 0.0, 0.62962962...</td>\n",
       "      <td>[0.2875, 0.3509259259259259, -0.17121212121212...</td>\n",
       "      <td>[0.3333333333333333, 0.8, 0.6296296296296297, ...</td>\n",
       "      <td>0.158254</td>\n",
       "      <td>0.521919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5407diary16.rtf</td>\n",
       "      <td>\\nInformation about diarist\\nDate of birth: 19...</td>\n",
       "      <td>Group 5</td>\n",
       "      <td>[information, diarist, date, birth, 1951, gend...</td>\n",
       "      <td>[\\nInformation about diarist\\nDate of birth: 1...</td>\n",
       "      <td>[0.0, 0.0, 0.020000000000000007, 0.425, 0.0, 0...</td>\n",
       "      <td>[0.0, 0.06666666666666667, 0.3983333333333333,...</td>\n",
       "      <td>[0.020000000000000007, 0.425, 0.125, 0.3, 0.37...</td>\n",
       "      <td>[0.06666666666666667, 0.3983333333333333, 1.0,...</td>\n",
       "      <td>0.117394</td>\n",
       "      <td>0.474066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number         Filename                                    everything_else  \\\n",
       "0       0  5407diary02.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "1       1  5407diary03.rtf  Information about diarist\\nDate of birth: 1966...   \n",
       "2       2  5407diary07.rtf  \\n\\nInformation about diarist\\nDate of birth: ...   \n",
       "3       3  5407diary08.rtf  Information about diarist\\nDate of birth: 1963...   \n",
       "4       4  5407diary09.rtf  Information about diarist\\nDate of birth: 1981...   \n",
       "5       5  5407diary10.rtf  Information about diarist\\nDate of birth: 1937...   \n",
       "6       6  5407diary13.rtf  Information about diarist\\nDate of birth: 1947...   \n",
       "7       7  5407diary14.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "8       8  5407diary15.rtf  Information about diarist\\nDate of birth: 1949...   \n",
       "9       9  5407diary16.rtf  \\nInformation about diarist\\nDate of birth: 19...   \n",
       "\n",
       "  Occupation                                     processed_text  \\\n",
       "0    Group 6  [information, diarist, date, birth, 1975, gend...   \n",
       "1    Group 6  [information, diarist, date, birth, 1966, gend...   \n",
       "2    Group 6  [information, diarist, date, birth, 1964, gend...   \n",
       "3    Group 6  [information, diarist, date, birth, 1963, gend...   \n",
       "4    Group 5  [information, diarist, date, birth, 1981, gend...   \n",
       "5    Group 5  [information, diarist, date, birth, 1937, gend...   \n",
       "6    Group 5  [information, diarist, date, birth, 1947, gend...   \n",
       "7    Group 5  [information, diarist, date, birth, 1964, gend...   \n",
       "8    Group 5  [information, diarist, date, birth, 1949, gend...   \n",
       "9    Group 5  [information, diarist, date, birth, 1951, gend...   \n",
       "\n",
       "                                      sent_tokenised  \\\n",
       "0  [\\n\\nInformation about diarist\\nDate of birth:...   \n",
       "1  [Information about diarist\\nDate of birth: 196...   \n",
       "2  [\\n\\nInformation about diarist\\nDate of birth:...   \n",
       "3  [Information about diarist\\nDate of birth: 196...   \n",
       "4  [Information about diarist\\nDate of birth: 198...   \n",
       "5  [Information about diarist\\nDate of birth: 193...   \n",
       "6  [Information about diarist\\nDate of birth: 194...   \n",
       "7  [\\nInformation about diarist\\nDate of birth: 1...   \n",
       "8  [Information about diarist\\nDate of birth: 194...   \n",
       "9  [\\nInformation about diarist\\nDate of birth: 1...   \n",
       "\n",
       "                                            polarity  \\\n",
       "0  [0.0, 0.125, 0.0, 0.0, 0.39999999999999997, 0....   \n",
       "1  [-0.19722222222222222, -0.3186111111111111, 0....   \n",
       "2  [0.11742424242424243, 0.0, 0.31145833333333334...   \n",
       "3  [0.3666666666666667, 0.5, 0.0, -0.051851851851...   \n",
       "4  [-0.0625, 0.5, -0.024999999999999994, 0.0, 0.0...   \n",
       "5  [0.0, -0.13333333333333333, 0.0, -0.1, 0.0, -0...   \n",
       "6  [0.0, -0.5, 0.0, 0.0, -0.1875, 0.375, 0.0, 0.5...   \n",
       "7  [0.0, 0.2, 0.10000000000000002, 0.0, 0.0499999...   \n",
       "8  [0.0, 0.0, 0.2875, 0.0, 0.3509259259259259, 0....   \n",
       "9  [0.0, 0.0, 0.020000000000000007, 0.425, 0.0, 0...   \n",
       "\n",
       "                                        subjectivity  \\\n",
       "0  [0.0, 0.375, 0.125, 0.17777777777777778, 0.466...   \n",
       "1  [0.3972222222222222, 0.5777777777777777, 0.0, ...   \n",
       "2  [0.28030303030303033, 0.0, 0.4166666666666667,...   \n",
       "3  [0.16666666666666666, 0.15, 0.0, 0.51851851851...   \n",
       "4  [0.3875, 0.5, 0.225, 0.0, 0.0, 0.25, 0.0, 0.25...   \n",
       "5  [0.0, 0.4666666666666666, 0.0, 0.0333333333333...   \n",
       "6  [0.0, 1.0, 0.0, 0.5333333333333333, 0.3125, 0....   \n",
       "7  [0.0, 0.85, 0.3833333333333333, 0.0, 0.2499999...   \n",
       "8  [0.3333333333333333, 0.0, 0.8, 0.0, 0.62962962...   \n",
       "9  [0.0, 0.06666666666666667, 0.3983333333333333,...   \n",
       "\n",
       "                                        pol_filtered  \\\n",
       "0  [0.125, 0.39999999999999997, 0.183333333333333...   \n",
       "1  [-0.19722222222222222, -0.3186111111111111, 0....   \n",
       "2  [0.11742424242424243, 0.31145833333333334, 0.2...   \n",
       "3  [0.3666666666666667, 0.5, -0.05185185185185181...   \n",
       "4  [-0.0625, 0.5, -0.024999999999999994, 0.25, 0....   \n",
       "5  [-0.13333333333333333, -0.1, -0.14, 0.4, -0.06...   \n",
       "6  [-0.5, -0.1875, 0.375, 0.5, -0.324999999999999...   \n",
       "7  [0.2, 0.10000000000000002, 0.04999999999999999...   \n",
       "8  [0.2875, 0.3509259259259259, -0.17121212121212...   \n",
       "9  [0.020000000000000007, 0.425, 0.125, 0.3, 0.37...   \n",
       "\n",
       "                                       subj_filtered  pol_mean  subj_mean  \n",
       "0  [0.375, 0.125, 0.17777777777777778, 0.46666666...  0.116305   0.444695  \n",
       "1  [0.3972222222222222, 0.5777777777777777, 0.687...  0.082216   0.511248  \n",
       "2  [0.28030303030303033, 0.4166666666666667, 0.31...  0.197901   0.500509  \n",
       "3  [0.16666666666666666, 0.15, 0.5185185185185186...  0.113457   0.481575  \n",
       "4  [0.3875, 0.5, 0.225, 0.25, 0.25, 1.0, 0.333333...  0.148358   0.503865  \n",
       "5  [0.4666666666666666, 0.03333333333333333, 1.0,...  0.085418   0.454015  \n",
       "6  [1.0, 0.5333333333333333, 0.3125, 0.75, 0.5, 0...  0.165448   0.487512  \n",
       "7  [0.85, 0.3833333333333333, 0.24999999999999997...  0.123176   0.492388  \n",
       "8  [0.3333333333333333, 0.8, 0.6296296296296297, ...  0.158254   0.521919  \n",
       "9  [0.06666666666666667, 0.3983333333333333, 1.0,...  0.117394   0.474066  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "oc_foot_mouth['pol_mean'] = oc_foot_mouth['pol_filtered'].apply(np.mean)\n",
    "oc_foot_mouth['subj_mean'] = oc_foot_mouth['subj_filtered'].apply(np.mean)\n",
    "\n",
    "#oc_foot_mouth.drop('polarity', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('subjectivity', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('pol_filtered', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('subj_filtered', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('pol_mean', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('subj_mean', axis=1, inplace=True)\n",
    "#oc_foot_mouth.drop('sent_tokenised', axis=1, inplace=True)\n",
    "\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 1']\n",
    "group4_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 4']\n",
    "group2_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 2']\n",
    "group3_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 3']\n",
    "group5_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 5']\n",
    "group6_foot = oc_foot_mouth[oc_foot_mouth['Occupation'] == 'Group 6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score averages are: \n",
      "0.08458962440621778\n",
      "0.4730428549538535\n",
      " \n",
      " Group 1 averages are: \n",
      "0.07660359635291054\n",
      "0.4556586795690575\n",
      " \n",
      " Group 4 averages are: \n",
      "0.08588717471431488\n",
      "0.4728307087663699\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.08211729314421548\n",
      "0.4786862218230877\n",
      " \n",
      "Group3:\n",
      "0.07161054306789462\n",
      "0.4738004165942562\n",
      " \n",
      "Group5:\n",
      "0.09657965994035568\n",
      "0.48024687682273853\n",
      " \n",
      "Group6:\n",
      "0.09659313260314527\n",
      "0.4737395089878944\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth[\"pol_mean\"].mean())\n",
    "print(oc_foot_mouth[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot[\"pol_mean\"].mean())\n",
    "print(group1_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot[\"pol_mean\"].mean())\n",
    "print(group4_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot[\"pol_mean\"].mean())\n",
    "print(group2_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot[\"pol_mean\"].mean())\n",
    "print(group3_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot[\"pol_mean\"].mean())\n",
    "print(group5_foot[\"subj_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot[\"pol_mean\"].mean())\n",
    "print(group6_foot[\"subj_mean\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it seems as if the unexpxectedness might just be due to the dataset and not an analysis error. But just to be safe we will also try another sentiment analysis tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that VADER is a generally more accurate sentiment analysis tool (see [here](https://investigate.ai/investigating-sentiment-analysis/comparing-sentiment-analysis-tools/))\n",
    "\n",
    "So we will try our analysis with this tool instead! First step is importing modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L_Pel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Now how it works on trivial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.8553}\n",
      "{'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.7644}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(\"Textblob is just super. I love it!\"))\n",
    "print(sia.polarity_scores(\"Cabbages are the worst. Say no to cabbages!\"))\n",
    "print(sia.polarity_scores(\"Paris is the capital of France\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to Note:**\n",
    "\n",
    "1) Works better WITH punctuation e.g. ! intensifies the emotion of the sentence\n",
    "\n",
    "2) Capitalisation works better with this e.g HOT > hot\n",
    "\n",
    "3) KEEP DEGREE MODIFIERS AND NEGATIVES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore will be trying this first on the RAW data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(content):\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    return sia_scores['compound']\n",
    "\n",
    "oc_foot_mouth[\"Compound\"] = oc_foot_mouth['everything_else'].apply(lambda x : get_scores(x))\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uhh... again really really strange for the sentiments to be so strong. Let's take a deeper look at what's going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.056, 'neu': 0.803, 'pos': 0.141, 'compound': 1.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc_foot_mouth[\"VADER\"] = oc_foot_mouth['everything_else'].apply(lambda x : sia.polarity_scores(x))\n",
    "oc_foot_mouth.loc[10, 'VADER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it looks like it isn't summing the sentiments properly so will have to try manually doing this instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Sentence Tokenised data\n",
    "\n",
    "We will first try analysing by sentence (the closest to the RAW dataset) using the same methodologies as with the Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_foot_mouth['sent_tokenised'] = oc_foot_mouth.apply(lambda row: nltk.sent_tokenize(row['everything_else']), axis=1)\n",
    "\n",
    "def get_scores(content):\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    return sia_scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating polarities and filtering out the 0.0 values\n",
    "oc_foot_mouth['sent_vader'] = oc_foot_mouth['sent_tokenised'].apply(lambda x : [get_scores(y)for y in x])\n",
    "oc_foot_mouth['filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['sent_vader']]\n",
    "\n",
    "# Finding the mean of both the filtered and unfiltered polarities\n",
    "oc_foot_mouth['vad_mean'] = oc_foot_mouth['sent_vader'].apply(np.mean) \n",
    "oc_foot_mouth['filt_vad_mean'] = oc_foot_mouth['filtered'].apply(np.mean) \n",
    "\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06949687176063819\n",
      "0.1150819329965901\n"
     ]
    }
   ],
   "source": [
    "print(oc_foot_mouth['vad_mean'].mean())\n",
    "print(oc_foot_mouth['filt_vad_mean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Fully-Processed Data\n",
    "\n",
    "And now doing the means on the original completely processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating polarities and filtering out the 0.0 values\n",
    "oc_foot_mouth['proc_vader'] = oc_foot_mouth['processed_text'].apply(lambda x : [get_scores(y)for y in x])\n",
    "oc_foot_mouth['proc_filtered'] = [list(filter(lambda x: x != 0, sublist)) for sublist in oc_foot_mouth['proc_vader']]\n",
    "\n",
    "# Finding the mean of both the filtered and unfiltered polarities\n",
    "oc_foot_mouth['proc_vad_mean'] = oc_foot_mouth['proc_vader'].apply(np.mean)\n",
    "oc_foot_mouth['proc_filt_vad_mean'] = oc_foot_mouth['proc_filtered'].apply(np.mean)\n",
    "\n",
    "oc_foot_mouth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oc_foot_mouth['proc_vad_mean'].mean())\n",
    "print(oc_foot_mouth['proc_filt_vad_mean'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Comparisons\n",
    "\n",
    "And finally doing comparisons of all of the VADER sentiment analyses done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Unfiltered Score averages are: \n",
      "0.06949687176063819\n",
      " \n",
      " Group 1 averages are: \n",
      "0.06209566099468896\n",
      " \n",
      " Group 4 averages are: \n",
      "0.06781730023153121\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.06753245052948226\n",
      " \n",
      "Group3:\n",
      "0.07133954471656889\n",
      " \n",
      "Group5:\n",
      "0.0731050334750517\n",
      " \n",
      "Group6:\n",
      "0.07491307704990476\n"
     ]
    }
   ],
   "source": [
    "# Sentence Data (unfiltered)\n",
    "\n",
    "print(\"Sentence Unfiltered Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth[\"vad_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot[\"vad_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot[\"vad_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot[\"vad_mean\"].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot[\"vad_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot[\"vad_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot[\"vad_mean\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Filtered Score averages are: \n",
      "0.1150819329965901\n",
      " \n",
      " Group 1 averages are: \n",
      "0.12500756430242138\n",
      " \n",
      " Group 4 averages are: \n",
      "0.10790695828077973\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.10536974116136351\n",
      " \n",
      "Group3:\n",
      "0.11395831371874353\n",
      " \n",
      "Group5:\n",
      "0.12239513557360256\n",
      " \n",
      "Group6:\n",
      "0.11421255089606336\n"
     ]
    }
   ],
   "source": [
    "# Sentence Data (filtered)\n",
    "\n",
    "print(\"Sentence Filtered Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth['filt_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot['filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot['filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot['filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot[\"filt_vad_mean\"].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot['filt_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot['filt_vad_mean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Unfiltered Score averages are: \n",
      "0.010651155968645912\n",
      " \n",
      " Group 1 averages are: \n",
      "0.010430390635996496\n",
      " \n",
      " Group 4 averages are: \n",
      "0.009178703520318595\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.011188353932942423\n",
      " \n",
      "Group3:\n",
      "0.010140732881883252\n",
      " \n",
      "Group5:\n",
      "0.011453328422140465\n",
      " \n",
      "Group6:\n",
      "0.012082437170911862\n"
     ]
    }
   ],
   "source": [
    "# Processed Data (unfiltered)\n",
    "\n",
    "print(\"Processed Unfiltered Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth['proc_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot['proc_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot['proc_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot['proc_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot['proc_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot['proc_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot['proc_vad_mean'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Filtered Score averages are: \n",
      "0.09082551868902182\n",
      " \n",
      " Group 1 averages are: \n",
      "0.09741171618892061\n",
      " \n",
      " Group 4 averages are: \n",
      "0.07716526516106408\n",
      " \n",
      " Other averages are:\n",
      " \n",
      "Group2:\n",
      "0.09077748021238714\n",
      " \n",
      "Group3:\n",
      "0.08688578256118008\n",
      " \n",
      "Group5:\n",
      "0.10130420217906953\n",
      " \n",
      "Group6:\n",
      "0.09098646502316536\n"
     ]
    }
   ],
   "source": [
    "# Processed Data (filtered)\n",
    "\n",
    "print(\"Processed Filtered Score averages are: \")\n",
    "\n",
    "print(oc_foot_mouth['proc_filt_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\" Group 1 averages are: \")\n",
    "print(group1_foot['proc_filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Group 4 averages are: \")\n",
    "print(group4_foot['proc_filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" Other averages are:\")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group2:\")\n",
    "print(group2_foot['proc_filt_vad_mean'].mean())\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group3:\")\n",
    "print(group3_foot['proc_filt_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Group5:\")\n",
    "print(group5_foot['proc_filt_vad_mean'].mean())\n",
    "\n",
    "print(\" \")\n",
    "print(\"Group6:\")\n",
    "print(group6_foot['proc_filt_vad_mean'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
